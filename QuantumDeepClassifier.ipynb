{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzUDWqw1hvn1Me+4t0V8UO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarceloClaro/QuantumDeepClassifier/blob/main/QuantumDeepClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit\n",
        "!pip install pennylane\n",
        "!pip install tensorflow-quantum\n",
        "!pip install matplotlib\n",
        "!pip install pillow\n"
      ],
      "metadata": {
        "id": "rFFY-BPO5Pm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sim, podemos ajustar o código para visualizar as imagens das classes extraídas do arquivo ZIP. Aqui está o passo a passo para carregar e exibir as imagens:\n",
        "\n",
        "---\n",
        "\n",
        "### Código Ajustado para Visualizar Imagens\n",
        "\n",
        "```python\n",
        "import zipfile\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Caminho do arquivo zip carregado\n",
        "zip_path = '/content/melanomas.zip'\n",
        "extract_path = '/content/melanomas'\n",
        "\n",
        "# Extraindo o arquivo zip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Listando os arquivos extraídos\n",
        "extracted_files = []\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    for file in files:\n",
        "        if file.endswith((\".jpg\", \".png\")):  # Verificar tipos de arquivos de imagem\n",
        "            extracted_files.append(os.path.join(root, file))\n",
        "\n",
        "# Mostrando a estrutura dos arquivos\n",
        "print(f\"Número total de imagens: {len(extracted_files)}\")\n",
        "print(\"Exemplo de arquivos:\", extracted_files[:10])\n",
        "\n",
        "# Função para visualizar imagens por classe\n",
        "def visualize_images(files, n=5):\n",
        "    \"\"\"Visualizar as primeiras N imagens de cada classe\"\"\"\n",
        "    classes = {}\n",
        "    for file in files:\n",
        "        class_name = os.path.basename(os.path.dirname(file))\n",
        "        if class_name not in classes:\n",
        "            classes[class_name] = []\n",
        "        classes[class_name].append(file)\n",
        "\n",
        "    for class_name, images in classes.items():\n",
        "        print(f\"Classe: {class_name} | Total de imagens: {len(images)}\")\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.suptitle(f\"Exemplos da classe: {class_name}\")\n",
        "        for i, img_path in enumerate(images[:n]):\n",
        "            img = Image.open(img_path)\n",
        "            plt.subplot(1, n, i + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"{class_name} {i+1}\")\n",
        "        plt.show()\n",
        "\n",
        "# Visualizar as imagens\n",
        "visualize_images(extracted_files, n=5)  # Visualiza as primeiras 5 imagens de cada classe\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Explicação do Código\n",
        "1. **Extração do ZIP**:\n",
        "   - O código extrai as imagens para o diretório especificado (`/content/melanomas`).\n",
        "2. **Filtragem de Arquivos**:\n",
        "   - Apenas arquivos com extensões de imagem (`.jpg`, `.png`) são incluídos.\n",
        "3. **Organização por Classe**:\n",
        "   - As imagens são agrupadas com base no nome da subpasta (representando as classes).\n",
        "4. **Visualização**:\n",
        "   - Para cada classe, as primeiras `n` imagens são exibidas em um gráfico.\n",
        "\n",
        "---\n",
        "\n",
        "### Resultados Esperados\n",
        "- Para cada classe encontrada no diretório extraído, o código exibe uma linha de imagens com títulos indicando a classe e o índice da imagem.\n",
        "\n",
        "Teste o código, e me avise se precisar de mais ajustes ou se houver dúvidas! 😊"
      ],
      "metadata": {
        "id": "jFku01OHyA4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Caminho do arquivo zip carregado\n",
        "zip_path = '/content/melanomas.zip'\n",
        "extract_path = '/content/melanomas'\n",
        "\n",
        "# Extraindo o arquivo zip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Listando os arquivos extraídos\n",
        "extracted_files = []\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    for file in files:\n",
        "        if file.endswith((\".jpg\", \".png\")):  # Verificar tipos de arquivos de imagem\n",
        "            extracted_files.append(os.path.join(root, file))\n",
        "\n",
        "# Mostrando a estrutura dos arquivos\n",
        "print(f\"Número total de imagens: {len(extracted_files)}\")\n",
        "print(\"Exemplo de arquivos:\", extracted_files[:10])\n",
        "\n",
        "# Função para visualizar imagens por classe\n",
        "def visualize_images(files, n=5):\n",
        "    \"\"\"Visualizar as primeiras N imagens de cada classe\"\"\"\n",
        "    classes = {}\n",
        "    for file in files:\n",
        "        class_name = os.path.basename(os.path.dirname(file))\n",
        "        if class_name not in classes:\n",
        "            classes[class_name] = []\n",
        "        classes[class_name].append(file)\n",
        "\n",
        "    for class_name, images in classes.items():\n",
        "        print(f\"Classe: {class_name} | Total de imagens: {len(images)}\")\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.suptitle(f\"Exemplos da classe: {class_name}\")\n",
        "        for i, img_path in enumerate(images[:n]):\n",
        "            img = Image.open(img_path)\n",
        "            plt.subplot(1, n, i + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"{class_name} {i+1}\")\n",
        "        plt.show()\n",
        "\n",
        "# Visualizar as imagens\n",
        "visualize_images(extracted_files, n=5)  # Visualiza as primeiras 5 imagens de cada classe\n"
      ],
      "metadata": {
        "id": "db0hdXbdxnIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sim, podemos ajustar o código para:\n",
        "\n",
        "1. **Visualizar a imagem original e redimensionada**.\n",
        "2. **Salvar os dados redimensionados em um DataFrame e exportá-lo para um arquivo, se necessário.**\n",
        "\n",
        "---\n",
        "\n",
        "### Código Ajustado para Visualização e Criação de DataFrame\n",
        "\n",
        "```python\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Função para processar e redimensionar imagens\n",
        "def process_and_visualize_image(image_path, resize_to=(64, 64)):\n",
        "    \"\"\"Processa a imagem, redimensiona e visualiza\"\"\"\n",
        "    # Abrir a imagem\n",
        "    image = Image.open(image_path)\n",
        "    \n",
        "    # Redimensionar a imagem\n",
        "    image_resized = image.resize(resize_to)\n",
        "    \n",
        "    # Converter para array normalizado\n",
        "    image_array = np.array(image_resized) / 255.0\n",
        "\n",
        "    # Visualizar a imagem original e redimensionada\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Imagem Original\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(image_resized)\n",
        "    plt.title(f\"Imagem Redimensionada {resize_to}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    return image_array\n",
        "\n",
        "# Exemplo com a primeira imagem\n",
        "sample_image_path = extracted_files[0]\n",
        "processed_image = process_and_visualize_image(sample_image_path)\n",
        "\n",
        "# Criar um DataFrame com os dados redimensionados\n",
        "def create_dataframe(image_paths, resize_to=(64, 64)):\n",
        "    \"\"\"Processa imagens e cria um DataFrame com arrays normalizados\"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    for image_path in image_paths:\n",
        "        # Processar a imagem\n",
        "        image_resized = Image.open(image_path).resize(resize_to)\n",
        "        image_array = np.array(image_resized).flatten() / 255.0  # Achatar a matriz\n",
        "        \n",
        "        # Obter o rótulo da classe\n",
        "        label = os.path.basename(os.path.dirname(image_path))\n",
        "        \n",
        "        # Adicionar ao dataset\n",
        "        data.append(image_array)\n",
        "        labels.append(label)\n",
        "    \n",
        "    # Criar o DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    df['label'] = labels\n",
        "    return df\n",
        "\n",
        "# Criar o DataFrame com todas as imagens\n",
        "df = create_dataframe(extracted_files)\n",
        "\n",
        "# Visualizar parte do DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Salvar o DataFrame em um arquivo CSV\n",
        "output_path = \"processed_images.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"DataFrame salvo em: {output_path}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Explicação do Código\n",
        "\n",
        "1. **Visualização da Imagem**:\n",
        "   - A função `process_and_visualize_image` exibe a imagem original e a redimensionada lado a lado.\n",
        "\n",
        "2. **Criação do DataFrame**:\n",
        "   - Cada imagem é convertida para um array 1D (achatar a matriz).\n",
        "   - Os arrays são armazenados como linhas no DataFrame, com uma coluna adicional para os rótulos das classes.\n",
        "\n",
        "3. **Exportação para CSV**:\n",
        "   - O DataFrame é salvo em um arquivo CSV (`processed_images.csv`), para ser reutilizado posteriormente.\n",
        "\n",
        "---\n",
        "\n",
        "### Resultados Esperados\n",
        "- Você verá a imagem original e redimensionada para confirmar o processo de redimensionamento.\n",
        "- O DataFrame conterá os dados de todas as imagens redimensionadas e seus rótulos de classe.\n",
        "- O arquivo CSV permitirá reutilizar os dados sem necessidade de processar as imagens novamente.\n",
        "\n",
        "Teste o código e informe se precisar de mais ajustes! 😊"
      ],
      "metadata": {
        "id": "6PgjAaxZyotH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Função para processar e redimensionar imagens\n",
        "def process_and_visualize_image(image_path, resize_to=(64, 64)):\n",
        "    \"\"\"Processa a imagem, redimensiona e visualiza\"\"\"\n",
        "    # Abrir a imagem\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Redimensionar a imagem\n",
        "    image_resized = image.resize(resize_to)\n",
        "\n",
        "    # Converter para array normalizado\n",
        "    image_array = np.array(image_resized) / 255.0\n",
        "\n",
        "    # Visualizar a imagem original e redimensionada\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Imagem Original\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(image_resized)\n",
        "    plt.title(f\"Imagem Redimensionada {resize_to}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    return image_array\n",
        "\n",
        "# Exemplo com a primeira imagem\n",
        "sample_image_path = extracted_files[0]\n",
        "processed_image = process_and_visualize_image(sample_image_path)\n",
        "\n",
        "# Criar um DataFrame com os dados redimensionados\n",
        "def create_dataframe(image_paths, resize_to=(64, 64)):\n",
        "    \"\"\"Processa imagens e cria um DataFrame com arrays normalizados\"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    for image_path in image_paths:\n",
        "        # Processar a imagem\n",
        "        image_resized = Image.open(image_path).resize(resize_to)\n",
        "        image_array = np.array(image_resized).flatten() / 255.0  # Achatar a matriz\n",
        "\n",
        "        # Obter o rótulo da classe\n",
        "        label = os.path.basename(os.path.dirname(image_path))\n",
        "\n",
        "        # Adicionar ao dataset\n",
        "        data.append(image_array)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Criar o DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    df['label'] = labels\n",
        "    return df\n",
        "\n",
        "# Criar o DataFrame com todas as imagens\n",
        "df = create_dataframe(extracted_files)\n",
        "\n",
        "# Visualizar parte do DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Salvar o DataFrame em um arquivo CSV\n",
        "output_path = \"processed_images.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"DataFrame salvo em: {output_path}\")\n"
      ],
      "metadata": {
        "id": "aw1NqaInzH_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está o código ajustado para visualizar os processos, a normalização das imagens e salvar os dados processados em um DataFrame para análise posterior.\n",
        "\n",
        "---\n",
        "\n",
        "### Código Ajustado com Visualização e DataFrame\n",
        "\n",
        "```python\n",
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Caminho das pastas para cada classe\n",
        "class_dirs = [os.path.join(extract_path, \"benigno\"), os.path.join(extract_path, \"maligno\")]\n",
        "image_size = (64, 64)\n",
        "\n",
        "# Função para processar e visualizar imagens\n",
        "def process_images_with_visualization(image_paths, image_size, n_visualizations=5):\n",
        "    images = []\n",
        "    labels = []\n",
        "    visualized = 0  # Contador de imagens visualizadas\n",
        "    for image_path in image_paths:\n",
        "        # Identificando a classe a partir do caminho\n",
        "        label = os.path.basename(os.path.dirname(image_path))\n",
        "        \n",
        "        # Abrindo, redimensionando e normalizando a imagem\n",
        "        image = Image.open(image_path)\n",
        "        image_resized = image.resize(image_size)\n",
        "        image_array = np.array(image_resized) / 255.0  # Normalização\n",
        "        \n",
        "        # Adicionar ao dataset\n",
        "        images.append(image_array)\n",
        "        labels.append(label)\n",
        "        \n",
        "        # Visualizar algumas imagens\n",
        "        if visualized < n_visualizations:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(image)\n",
        "            plt.title(\"Original\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(image_resized)\n",
        "            plt.title(f\"Redimensionada ({image_size}) e Normalizada\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "            print(f\"Array Normalizado ({image_array.shape}):\")\n",
        "            print(image_array[:5, :5, 0])  # Mostrando parte da matriz normalizada\n",
        "            visualized += 1\n",
        "\n",
        "    return np.array(images), labels\n",
        "\n",
        "# Coletando todas as imagens do diretório\n",
        "all_image_paths = [img for class_dir in class_dirs for img in glob.glob(f\"{class_dir}/*.jpg\")]\n",
        "\n",
        "# Processar imagens com visualização\n",
        "processed_images, labels = process_images_with_visualization(all_image_paths, image_size)\n",
        "\n",
        "# Criar DataFrame para salvar os dados\n",
        "def create_dataframe(images, labels):\n",
        "    flattened_images = [img.flatten() for img in images]  # Achatar as imagens\n",
        "    df = pd.DataFrame(flattened_images)\n",
        "    df['label'] = labels\n",
        "    return df\n",
        "\n",
        "# Criar e salvar o DataFrame\n",
        "df = create_dataframe(processed_images, labels)\n",
        "print(\"Amostra do DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "output_path = \"processed_images_with_labels.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"DataFrame salvo em: {output_path}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### O Que Esse Código Faz\n",
        "1. **Visualização das Imagens**:\n",
        "   - Exibe as imagens originais e redimensionadas lado a lado.\n",
        "   - Mostra a matriz normalizada das imagens para verificar os valores (entre 0 e 1).\n",
        "\n",
        "2. **Processamento das Imagens**:\n",
        "   - Redimensiona as imagens para \\(64 \\times 64\\) pixels.\n",
        "   - Normaliza os valores de pixel para o intervalo \\([0, 1]\\).\n",
        "\n",
        "3. **Criação do DataFrame**:\n",
        "   - Achata as imagens redimensionadas em vetores 1D.\n",
        "   - Adiciona uma coluna com os rótulos correspondentes.\n",
        "\n",
        "4. **Exportação para CSV**:\n",
        "   - Salva o DataFrame com os dados normalizados e os rótulos em um arquivo CSV.\n",
        "\n",
        "---\n",
        "\n",
        "### Resultados Esperados\n",
        "- Visualização de algumas imagens originais e redimensionadas.\n",
        "- Matriz normalizada exibida para análise.\n",
        "- DataFrame contendo:\n",
        "  - Vetores de pixels normalizados (uma linha por imagem).\n",
        "  - Rótulos das classes como uma coluna separada.\n",
        "- Arquivo `processed_images_with_labels.csv` contendo o DataFrame para análise posterior.\n",
        "\n",
        "Teste o código e informe os resultados! 😊"
      ],
      "metadata": {
        "id": "me-sYB307FFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Caminho das pastas para cada classe\n",
        "class_dirs = [os.path.join(extract_path, \"benigno\"), os.path.join(extract_path, \"maligno\")]\n",
        "image_size = (64, 64)\n",
        "\n",
        "# Função para processar e visualizar imagens\n",
        "def process_images_with_visualization(image_paths, image_size, n_visualizations=5):\n",
        "    images = []\n",
        "    labels = []\n",
        "    visualized = 0  # Contador de imagens visualizadas\n",
        "    for image_path in image_paths:\n",
        "        # Identificando a classe a partir do caminho\n",
        "        label = os.path.basename(os.path.dirname(image_path))\n",
        "\n",
        "        # Abrindo, redimensionando e normalizando a imagem\n",
        "        image = Image.open(image_path)\n",
        "        image_resized = image.resize(image_size)\n",
        "        image_array = np.array(image_resized) / 255.0  # Normalização\n",
        "\n",
        "        # Adicionar ao dataset\n",
        "        images.append(image_array)\n",
        "        labels.append(label)\n",
        "\n",
        "        # Visualizar algumas imagens\n",
        "        if visualized < n_visualizations:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(image)\n",
        "            plt.title(\"Original\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(image_resized)\n",
        "            plt.title(f\"Redimensionada ({image_size}) e Normalizada\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "            print(f\"Array Normalizado ({image_array.shape}):\")\n",
        "            print(image_array[:5, :5, 0])  # Mostrando parte da matriz normalizada\n",
        "            visualized += 1\n",
        "\n",
        "    return np.array(images), labels\n",
        "\n",
        "# Coletando todas as imagens do diretório\n",
        "all_image_paths = [img for class_dir in class_dirs for img in glob.glob(f\"{class_dir}/*.jpg\")]\n",
        "\n",
        "# Processar imagens com visualização\n",
        "processed_images, labels = process_images_with_visualization(all_image_paths, image_size)\n",
        "\n",
        "# Criar DataFrame para salvar os dados\n",
        "def create_dataframe(images, labels):\n",
        "    flattened_images = [img.flatten() for img in images]  # Achatar as imagens\n",
        "    df = pd.DataFrame(flattened_images)\n",
        "    df['label'] = labels\n",
        "    return df\n",
        "\n",
        "# Criar e salvar o DataFrame\n",
        "df = create_dataframe(processed_images, labels)\n",
        "print(\"Amostra do DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "output_path = \"processed_images_with_labels.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"DataFrame salvo em: {output_path}\")\n"
      ],
      "metadata": {
        "id": "rk1TQzrJ6HJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para incluir visualizações, informações sobre os dados processados e salvá-los em um DataFrame após a aplicação de `LabelEncoder` e `train_test_split`, aqui está o código ajustado:\n",
        "\n",
        "---\n",
        "\n",
        "### Código Ajustado\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convertendo rótulos de texto para valores numéricos\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Transformando imagens em vetores unidimensionais\n",
        "flattened_images = processed_images.reshape(processed_images.shape[0], -1)\n",
        "\n",
        "# Dividindo os dados em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(flattened_images, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Visualização de amostras do conjunto de treino\n",
        "def visualize_data_split(X, y, label_encoder, n=5):\n",
        "    \"\"\"Visualiza algumas imagens do conjunto de treino ou teste\"\"\"\n",
        "    for i in range(n):\n",
        "        image = X[i].reshape(64, 64, 3)  # Restaurar formato original\n",
        "        label = label_encoder.inverse_transform([y[i]])[0]  # Decodificar rótulo numérico para texto\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"Classe: {label}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "        print(f\"Matriz Normalizada (amostra {i+1}):\")\n",
        "        print(image[:5, :5, 0])  # Exibe uma parte da matriz normalizada\n",
        "\n",
        "# Visualizar amostras do conjunto de treino\n",
        "print(\"Amostras do conjunto de treino:\")\n",
        "visualize_data_split(X_train, y_train, label_encoder, n=5)\n",
        "\n",
        "# Criar DataFrame com os dados de treino\n",
        "def create_dataframe(X, y, label_encoder):\n",
        "    \"\"\"Cria um DataFrame com as imagens achatadas e os rótulos\"\"\"\n",
        "    df = pd.DataFrame(X)\n",
        "    df['label'] = label_encoder.inverse_transform(y)  # Adicionar os rótulos decodificados\n",
        "    return df\n",
        "\n",
        "# Criar DataFrames para treino e teste\n",
        "train_df = create_dataframe(X_train, y_train, label_encoder)\n",
        "test_df = create_dataframe(X_test, y_test, label_encoder)\n",
        "\n",
        "# Salvar os DataFrames em arquivos CSV\n",
        "train_df.to_csv(\"train_data.csv\", index=False)\n",
        "test_df.to_csv(\"test_data.csv\", index=False)\n",
        "\n",
        "print(\"DataFrame de treino salvo como 'train_data.csv'\")\n",
        "print(\"DataFrame de teste salvo como 'test_data.csv'\")\n",
        "\n",
        "# Exibir amostras dos DataFrames\n",
        "print(\"Amostra do DataFrame de treino:\")\n",
        "print(train_df.head())\n",
        "print(\"Amostra do DataFrame de teste:\")\n",
        "print(test_df.head())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### O Que Esse Código Faz\n",
        "\n",
        "1. **LabelEncoder**:\n",
        "   - Converte rótulos textuais (ex.: \"benigno\", \"maligno\") em valores numéricos.\n",
        "\n",
        "2. **Flattened Images**:\n",
        "   - As imagens redimensionadas \\(64 \\times 64 \\times 3\\) são transformadas em vetores unidimensionais.\n",
        "\n",
        "3. **Divisão de Dados**:\n",
        "   - Usa `train_test_split` para dividir as imagens e os rótulos em conjuntos de treino e teste.\n",
        "\n",
        "4. **Visualização**:\n",
        "   - Exibe algumas imagens do conjunto de treino com seus rótulos decodificados.\n",
        "   - Mostra parte das matrizes normalizadas das imagens.\n",
        "\n",
        "5. **Criação de DataFrames**:\n",
        "   - Salva os dados de treino e teste (incluindo rótulos) em DataFrames separados.\n",
        "   - Exporta os DataFrames para arquivos CSV (`train_data.csv` e `test_data.csv`).\n",
        "\n",
        "---\n",
        "\n",
        "### Resultados Esperados\n",
        "1. **Visualização**:\n",
        "   - Você verá as primeiras 5 imagens do conjunto de treino com os rótulos decodificados.\n",
        "   - Matriz normalizada será exibida para cada imagem.\n",
        "\n",
        "2. **Arquivos CSV**:\n",
        "   - Os dados de treino e teste serão salvos como arquivos CSV:\n",
        "     - `train_data.csv`\n",
        "     - `test_data.csv`\n",
        "\n",
        "3. **Amostra de DataFrames**:\n",
        "   - Os DataFrames com os dados processados serão exibidos para confirmação.\n",
        "\n",
        "Teste o código e me avise se precisar de mais ajustes! 😊"
      ],
      "metadata": {
        "id": "TROm3P5W8Isd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convertendo rótulos de texto para valores numéricos\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Transformando imagens em vetores unidimensionais\n",
        "flattened_images = processed_images.reshape(processed_images.shape[0], -1)\n",
        "\n",
        "# Dividindo os dados em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(flattened_images, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Visualização de amostras do conjunto de treino\n",
        "def visualize_data_split(X, y, label_encoder, n=5):\n",
        "    \"\"\"Visualiza algumas imagens do conjunto de treino ou teste\"\"\"\n",
        "    for i in range(n):\n",
        "        image = X[i].reshape(64, 64, 3)  # Restaurar formato original\n",
        "        label = label_encoder.inverse_transform([y[i]])[0]  # Decodificar rótulo numérico para texto\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"Classe: {label}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "        print(f\"Matriz Normalizada (amostra {i+1}):\")\n",
        "        print(image[:5, :5, 0])  # Exibe uma parte da matriz normalizada\n",
        "\n",
        "# Visualizar amostras do conjunto de treino\n",
        "print(\"Amostras do conjunto de treino:\")\n",
        "visualize_data_split(X_train, y_train, label_encoder, n=5)\n",
        "\n",
        "# Criar DataFrame com os dados de treino\n",
        "def create_dataframe(X, y, label_encoder):\n",
        "    \"\"\"Cria um DataFrame com as imagens achatadas e os rótulos\"\"\"\n",
        "    df = pd.DataFrame(X)\n",
        "    df['label'] = label_encoder.inverse_transform(y)  # Adicionar os rótulos decodificados\n",
        "    return df\n",
        "\n",
        "# Criar DataFrames para treino e teste\n",
        "train_df = create_dataframe(X_train, y_train, label_encoder)\n",
        "test_df = create_dataframe(X_test, y_test, label_encoder)\n",
        "\n",
        "# Salvar os DataFrames em arquivos CSV\n",
        "train_df.to_csv(\"train_data.csv\", index=False)\n",
        "test_df.to_csv(\"test_data.csv\", index=False)\n",
        "\n",
        "print(\"DataFrame de treino salvo como 'train_data.csv'\")\n",
        "print(\"DataFrame de teste salvo como 'test_data.csv'\")\n",
        "\n",
        "# Exibir amostras dos DataFrames\n",
        "print(\"Amostra do DataFrame de treino:\")\n",
        "print(train_df.head())\n",
        "print(\"Amostra do DataFrame de teste:\")\n",
        "print(test_df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "DPVnmGLZ69_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação do Código\n",
        "\n",
        "1. **Configuração do Dispositivo Quântico**:\n",
        "   - Foi utilizado o dispositivo `default.qubit` com 10 qubits.\n",
        "   - Os qubits servem como a base para realizar os cálculos quânticos e as simulações.\n",
        "\n",
        "2. **Embedding Quântico (`data_embedding`)**:\n",
        "   - Os valores clássicos das amostras (`features`) são incorporados no circuito quântico usando rotações \\( RY \\).\n",
        "   - Cada qubit recebe um valor de `features`, que é convertido em uma rotação.\n",
        "\n",
        "3. **Modelo Quântico (`quantum_model`)**:\n",
        "   - Após o embedding, o circuito aplica camadas de entanglement (`BasicEntanglerLayers`), que introduzem correlações entre os qubits.\n",
        "   - Finalmente, a expectativa \\( \\langle Z \\rangle \\) é medida no primeiro qubit.\n",
        "\n",
        "4. **Função `circuit`**:\n",
        "   - É um `QNode` que encapsula o modelo quântico. É usado para calcular as previsões baseadas nos pesos e nas features fornecidas.\n",
        "\n",
        "5. **Processamento de Amostras**:\n",
        "   - A função `process_samples`:\n",
        "     - Executa o circuito para cada amostra de entrada.\n",
        "     - Calcula a previsão (saída do circuito).\n",
        "     - Calcula o resíduo, ou seja, a diferença entre a previsão e o rótulo verdadeiro.\n",
        "\n",
        "6. **DataFrame**:\n",
        "   - Cada entrada no DataFrame contém:\n",
        "     - As `features` (valores normalizados da amostra).\n",
        "     - O `label` (rótulo verdadeiro).\n",
        "     - A `prediction` (saída do circuito quântico).\n",
        "     - O `residual` (erro entre a previsão e o rótulo verdadeiro).\n",
        "   - O DataFrame é salvo como `quantum_circuit_results.csv`.\n",
        "\n",
        "7. **Histograma**:\n",
        "   - A função `plot_histograms` exibe a distribuição das previsões para cada classe (0 e 1).\n",
        "   - Permite verificar se há separação clara entre as classes baseada nas saídas do circuito.\n",
        "\n",
        "8. **Circuitos das Amostras**:\n",
        "   - A função `plot_circuits` desenha o circuito usado para processar algumas amostras de cada classe.\n",
        "   - Utiliza `qml.draw(circuit)` para gerar a representação textual do circuito.\n",
        "\n",
        "---\n",
        "\n",
        "### Explicação dos Resultados\n",
        "\n",
        "1. **DataFrame**:\n",
        "   - Contém as previsões e os resíduos para cada amostra.\n",
        "   - Amostras exibidas:\n",
        "     - `prediction` é um número contínuo (ex.: 0.006515, 0.068560).\n",
        "     - `residual` indica a diferença entre a previsão e o rótulo (valores negativos mostram subestimação do rótulo pelo modelo).\n",
        "\n",
        "2. **Histograma**:\n",
        "   - O gráfico mostra a sobreposição das distribuições de previsão para as classes 0 e 1.\n",
        "   - No exemplo:\n",
        "     - A classe 0 está concentrada em previsões menores.\n",
        "     - A classe 1 tem algumas previsões muito maiores.\n",
        "\n",
        "3. **Circuitos**:\n",
        "   - Cada circuito mostrado inclui:\n",
        "     - As rotações \\( RY \\) com os valores de `features`.\n",
        "     - As camadas de entanglement (`BasicEntanglerLayers`) parametrizadas pelos pesos (`M0`).\n",
        "   - Cada circuito é diferente, pois os valores de `features` variam entre as amostras.\n",
        "\n",
        "---\n",
        "\n",
        "### Próximos Passos\n",
        "\n",
        "1. **Análise dos Resultados**:\n",
        "   - Se os resíduos forem altos, o modelo pode precisar de ajustes:\n",
        "     - Aumentar o número de camadas ou qubits.\n",
        "     - Melhorar os pesos iniciais.\n",
        "\n",
        "2. **Visualização Avançada**:\n",
        "   - Adicionar gráficos de dispersão (predição vs rótulo).\n",
        "   - Analisar resíduos em relação às classes.\n",
        "\n",
        "3. **Treinamento**:\n",
        "   - Incorporar uma etapa de otimização (treinamento) para ajustar os pesos do circuito e melhorar as previsões.\n",
        "\n",
        "Se precisar de ajuda com esses ajustes ou mais explicações, é só avisar! 😊"
      ],
      "metadata": {
        "id": "VemAfafo_VBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configuração do dispositivo quântico\n",
        "n_qubits = 10\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Função de embedding quântico\n",
        "def data_embedding(features, wires):\n",
        "    for i, wire in enumerate(wires):\n",
        "        qml.RY(features[i], wires=wire)\n",
        "\n",
        "# Função do modelo quântico\n",
        "def quantum_model(weights, features):\n",
        "    data_embedding(features, range(n_qubits))\n",
        "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "# QNode com Pennylane\n",
        "n_layers = 3\n",
        "weights_shape = (n_layers, n_qubits)\n",
        "weights = np.random.random(weights_shape)\n",
        "\n",
        "@qml.qnode(dev)\n",
        "def circuit(weights, features):\n",
        "    return quantum_model(weights, features)\n",
        "\n",
        "# Processar todas as amostras e armazenar no DataFrame\n",
        "def process_samples(X, y, n_qubits, weights):\n",
        "    data = []\n",
        "    for i in range(len(X)):\n",
        "        features = X[i][:n_qubits]\n",
        "        features = np.pad(features, (0, n_qubits - len(features))) if len(features) < n_qubits else features[:n_qubits]\n",
        "        prediction = float(circuit(weights, features))\n",
        "        residual = prediction - y[i]\n",
        "        data.append({\"features\": features.tolist(), \"label\": y[i], \"prediction\": prediction, \"residual\": residual})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Gerar o DataFrame\n",
        "df = process_samples(X_train, y_train, n_qubits, weights)\n",
        "\n",
        "# Visualizar parte do DataFrame\n",
        "print(\"Amostra do DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Salvar o DataFrame em um CSV\n",
        "df.to_csv(\"quantum_circuit_results.csv\", index=False)\n",
        "print(\"DataFrame salvo como 'quantum_circuit_results.csv'\")\n",
        "\n",
        "# Gráficos de histograma para cada classe\n",
        "def plot_histograms(df):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for label in df['label'].unique():\n",
        "        subset = df[df['label'] == label]\n",
        "        plt.hist(subset['prediction'], bins=20, alpha=0.7, label=f\"Classe {label}\")\n",
        "    plt.title(\"Distribuição das Previsões por Classe\")\n",
        "    plt.xlabel(\"Previsão\")\n",
        "    plt.ylabel(\"Frequência\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plotar histogramas\n",
        "plot_histograms(df)\n",
        "\n",
        "# Visualizar o circuito para cada classe usando qml.draw\n",
        "def plot_circuits(X, y, n_samples=2):\n",
        "    for label in np.unique(y):\n",
        "        indices = np.where(y == label)[0][:n_samples]\n",
        "        for idx in indices:\n",
        "            features = X[idx][:n_qubits]\n",
        "            features = np.pad(features, (0, n_qubits - len(features))) if len(features) < n_qubits else features[:n_qubits]\n",
        "            print(f\"Circuito para a amostra {idx} (Classe {label}):\")\n",
        "            drawer = qml.draw(circuit)(weights, features)\n",
        "            print(drawer)\n",
        "            print()\n",
        "\n",
        "# Plotar circuitos\n",
        "plot_circuits(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "Sd73-igo7OmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está o código ajustado que implementa os próximos passos propostos, incluindo o ajuste dos pesos, visualização avançada e armazenamento dos resultados no DataFrame.\n",
        "\n",
        "---\n",
        "\n",
        "### Código Ajustado com Otimização e Visualização\n",
        "\n",
        "```python\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "\n",
        "# Configuração do dispositivo quântico\n",
        "n_qubits = 10\n",
        "n_layers = 4  # Aumentar o número de camadas para maior expressividade\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Função de embedding quântico\n",
        "def data_embedding(features, wires):\n",
        "    for i, wire in enumerate(wires):\n",
        "        qml.RY(features[i], wires=wire)\n",
        "\n",
        "# Função do modelo quântico\n",
        "def quantum_model(weights, features):\n",
        "    data_embedding(features, range(n_qubits))\n",
        "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "# QNode com Pennylane\n",
        "weights_shape = (n_layers, n_qubits)\n",
        "\n",
        "@qml.qnode(dev)\n",
        "def circuit(weights, features):\n",
        "    return quantum_model(weights, features)\n",
        "\n",
        "# Função de custo\n",
        "def cost(weights, X, y):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        pred = circuit(weights, X[i])\n",
        "        loss += (pred - y[i])**2\n",
        "    return loss / len(X)\n",
        "\n",
        "# Inicialização de pesos\n",
        "weights = np.random.uniform(low=-0.1, high=0.1, size=weights_shape, requires_grad=True)\n",
        "\n",
        "# Configuração do otimizador\n",
        "opt = AdamOptimizer(stepsize=0.01)\n",
        "steps = 50  # Número de iterações\n",
        "\n",
        "# Dados de treino e teste (ajustar para suportar as dimensões corretas)\n",
        "X_train_resized = X_train[:, :n_qubits]  # Usar apenas os primeiros n_qubits features\n",
        "X_test_resized = X_test[:, :n_qubits]\n",
        "\n",
        "# Treinamento\n",
        "train_costs = []\n",
        "test_costs = []\n",
        "\n",
        "for step in range(steps):\n",
        "    weights = opt.step(lambda w: cost(w, X_train_resized, y_train), weights)\n",
        "    train_cost = cost(weights, X_train_resized, y_train)\n",
        "    test_cost = cost(weights, X_test_resized, y_test)\n",
        "    train_costs.append(train_cost)\n",
        "    test_costs.append(test_cost)\n",
        "    if step % 10 == 0:\n",
        "        print(f\"Step {step}/{steps}: Train Cost = {train_cost:.4f} | Test Cost = {test_cost:.4f}\")\n",
        "\n",
        "# Avaliação final\n",
        "final_train_cost = cost(weights, X_train_resized, y_train)\n",
        "final_test_cost = cost(weights, X_test_resized, y_test)\n",
        "print(f\"Custo final no conjunto de treino: {final_train_cost:.4f}\")\n",
        "print(f\"Custo final no conjunto de teste: {final_test_cost:.4f}\")\n",
        "\n",
        "# Previsões\n",
        "y_train_pred = [float(circuit(weights, x)) for x in X_train_resized]\n",
        "y_test_pred = [float(circuit(weights, x)) for x in X_test_resized]\n",
        "\n",
        "# Criação do DataFrame com resíduos\n",
        "df_results = pd.DataFrame({\n",
        "    \"train_features\": [x.tolist() for x in X_train_resized],\n",
        "    \"train_labels\": y_train,\n",
        "    \"train_predictions\": y_train_pred,\n",
        "    \"train_residuals\": [pred - y for pred, y in zip(y_train_pred, y_train)],\n",
        "    \"test_features\": [x.tolist() for x in X_test_resized],\n",
        "    \"test_labels\": y_test,\n",
        "    \"test_predictions\": y_test_pred,\n",
        "    \"test_residuals\": [pred - y for pred, y in zip(y_test_pred, y_test)]\n",
        "})\n",
        "\n",
        "# Salvar os resultados no DataFrame\n",
        "df_results.to_csv(\"quantum_model_results.csv\", index=False)\n",
        "print(\"Resultados salvos no arquivo 'quantum_model_results.csv'\")\n",
        "\n",
        "# Visualizações\n",
        "\n",
        "# 1. Gráfico de custo durante o treinamento\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(steps), train_costs, label=\"Custo de Treinamento\", color=\"blue\")\n",
        "plt.plot(range(steps), test_costs, label=\"Custo de Teste\", color=\"orange\")\n",
        "plt.title(\"Evolução do Custo Durante o Treinamento\")\n",
        "plt.xlabel(\"Passos\")\n",
        "plt.ylabel(\"Custo\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 2. Gráfico de dispersão (predição vs rótulo)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_train, y_train_pred, alpha=0.7, label=\"Treino\", color=\"blue\")\n",
        "plt.scatter(y_test, y_test_pred, alpha=0.7, label=\"Teste\", color=\"orange\")\n",
        "plt.title(\"Dispersão: Previsão vs Rótulo\")\n",
        "plt.xlabel(\"Rótulo Verdadeiro\")\n",
        "plt.ylabel(\"Previsão\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 3. Histograma de resíduos\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df_results[\"train_residuals\"], bins=20, alpha=0.7, label=\"Treino\", color=\"blue\")\n",
        "plt.hist(df_results[\"test_residuals\"], bins=20, alpha=0.7, label=\"Teste\", color=\"orange\")\n",
        "plt.title(\"Distribuição dos Resíduos\")\n",
        "plt.xlabel(\"Resíduo\")\n",
        "plt.ylabel(\"Frequência\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Explicação do Código\n",
        "\n",
        "1. **Treinamento do Modelo**:\n",
        "   - Foi realizada uma etapa de otimização para ajustar os pesos do circuito quântico.\n",
        "   - Usamos o custo médio quadrático como métrica de erro.\n",
        "\n",
        "2. **Visualização**:\n",
        "   - **Gráfico de Custo**: Exibe a evolução do custo durante as iterações de treinamento.\n",
        "   - **Dispersão**: Mostra a relação entre as previsões do circuito e os rótulos verdadeiros.\n",
        "   - **Histograma de Resíduos**: Visualiza a distribuição dos erros (resíduos) entre as previsões e os rótulos.\n",
        "\n",
        "3. **Resultados no DataFrame**:\n",
        "   - Inclui previsões, resíduos e features para treino e teste.\n",
        "   - Salvo como `quantum_model_results.csv` para análise futura.\n",
        "\n",
        "---\n",
        "\n",
        "### Resultados Esperados\n",
        "\n",
        "- **Custo de Treinamento**:\n",
        "  - Deve diminuir com as iterações, indicando aprendizado do modelo.\n",
        "  \n",
        "- **Dispersão**:\n",
        "  - Pontos próximos à linha \\( y = x \\) indicam previsões corretas.\n",
        "\n",
        "- **Histograma**:\n",
        "  - Resíduos concentrados próximos a 0 indicam bom desempenho do modelo.\n",
        "\n",
        "Teste o código e visualize os gráficos e o DataFrame para confirmar os resultados! 😊"
      ],
      "metadata": {
        "id": "doIDRTIDAane"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação do Código\n",
        "\n",
        "O código é um pipeline para treinar e avaliar um modelo quântico de aprendizado de máquina. Ele combina métodos de otimização clássicos com computação quântica para processar dados e gerar previsões. Vamos detalhar cada etapa:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Configuração do Dispositivo Quântico**\n",
        "```python\n",
        "n_qubits = 10\n",
        "n_layers = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "```\n",
        "- **`n_qubits`**: Define o número de qubits usados no circuito quântico.\n",
        "- **`n_layers`**: Define o número de camadas de entanglement para maior expressividade do modelo.\n",
        "- **`qml.device`**: Configura o simulador quântico (`default.qubit`), que emula o comportamento de um processador quântico.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Embedding Quântico**\n",
        "```python\n",
        "def data_embedding(features, wires):\n",
        "    for i, wire in enumerate(wires):\n",
        "        qml.RY(features[i], wires=wire)\n",
        "```\n",
        "- Os dados clássicos (`features`) são convertidos em rotações \\( RY \\) nos qubits. Isso \"mapeia\" os dados clássicos para o espaço quântico, permitindo processamento no circuito.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Modelo Quântico**\n",
        "```python\n",
        "def quantum_model(weights, features):\n",
        "    data_embedding(features, range(n_qubits))\n",
        "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "```\n",
        "- **`data_embedding`**: Insere os dados no circuito.\n",
        "- **`BasicEntanglerLayers`**: Camadas parametrizadas que criam entanglement (correlações quânticas) entre os qubits. Essas camadas são ajustadas durante o treinamento.\n",
        "- **`expval(qml.PauliZ(0))`**: Mede a expectativa do operador \\( Z \\) no primeiro qubit, gerando uma saída contínua (previsão).\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Função do Circuito**\n",
        "```python\n",
        "@qml.qnode(dev)\n",
        "def circuit(weights, features):\n",
        "    return quantum_model(weights, features)\n",
        "```\n",
        "- O circuito encapsula o modelo quântico e é usado para calcular as previsões com base nos pesos e nas features fornecidas.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Função de Custo**\n",
        "```python\n",
        "def cost(weights, X, y):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        pred = circuit(weights, X[i])\n",
        "        loss += (pred - y[i])**2\n",
        "    return loss / len(X)\n",
        "```\n",
        "- **Erro Quadrático Médio**: Calcula o erro médio entre as previsões do circuito e os rótulos verdadeiros (`y`).\n",
        "- Essa função é usada pelo otimizador para ajustar os pesos do circuito.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Inicialização dos Pesos e Configuração do Otimizador**\n",
        "```python\n",
        "weights = np.random.uniform(low=-0.1, high=0.1, size=weights_shape, requires_grad=True)\n",
        "opt = AdamOptimizer(stepsize=0.01)\n",
        "steps = 50\n",
        "```\n",
        "- **`weights`**: Os pesos iniciais são aleatórios, com gradientes habilitados.\n",
        "- **`AdamOptimizer`**: Otimizador que ajusta os pesos para minimizar o custo.\n",
        "- **`steps`**: Define o número de iterações de treinamento.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Treinamento**\n",
        "```python\n",
        "for step in range(steps):\n",
        "    weights = opt.step(lambda w: cost(w, X_train_resized, y_train), weights)\n",
        "```\n",
        "- A cada iteração, o otimizador ajusta os pesos do circuito para minimizar o custo no conjunto de treinamento.\n",
        "- Os custos de treinamento e teste são registrados em listas (`train_costs` e `test_costs`).\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Avaliação Final**\n",
        "```python\n",
        "final_train_cost = cost(weights, X_train_resized, y_train)\n",
        "final_test_cost = cost(weights, X_test_resized, y_test)\n",
        "```\n",
        "- O custo final é calculado para verificar a performance do modelo nos conjuntos de treinamento e teste.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Previsões e Resultados**\n",
        "```python\n",
        "y_train_pred = [float(circuit(weights, x)) for x in X_train_resized]\n",
        "y_test_pred = [float(circuit(weights, x)) for x in X_test_resized]\n",
        "```\n",
        "- **Previsões**: O circuito é usado para gerar previsões para as amostras de treinamento e teste.\n",
        "- **Resíduos**: A diferença entre previsão e rótulo verdadeiro é calculada e salva no DataFrame.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Visualizações**\n",
        "#### **Gráfico de Custo**\n",
        "```python\n",
        "plt.plot(range(steps), train_costs, label=\"Custo de Treinamento\", color=\"blue\")\n",
        "plt.plot(range(steps), test_costs, label=\"Custo de Teste\", color=\"orange\")\n",
        "plt.title(\"Evolução do Custo Durante o Treinamento\")\n",
        "```\n",
        "- Mostra como o custo diminui ao longo do treinamento, indicando aprendizado do modelo.\n",
        "\n",
        "#### **Gráfico de Dispersão**\n",
        "```python\n",
        "plt.scatter(y_train, y_train_pred, alpha=0.7, label=\"Treino\", color=\"blue\")\n",
        "plt.scatter(y_test, y_test_pred, alpha=0.7, label=\"Teste\", color=\"orange\")\n",
        "plt.title(\"Dispersão: Previsão vs Rótulo\")\n",
        "```\n",
        "- Compara as previsões do circuito com os rótulos verdadeiros. Pontos próximos à linha \\( y = x \\) indicam previsões precisas.\n",
        "\n",
        "#### **Histograma de Resíduos**\n",
        "```python\n",
        "plt.hist(df_results[\"train_residuals\"], bins=20, alpha=0.7, label=\"Treino\", color=\"blue\")\n",
        "plt.hist(df_results[\"test_residuals\"], bins=20, alpha=0.7, label=\"Teste\", color=\"orange\")\n",
        "plt.title(\"Distribuição dos Resíduos\")\n",
        "```\n",
        "- Visualiza a distribuição dos erros. Resíduos próximos de zero indicam bom desempenho do modelo.\n",
        "\n",
        "---\n",
        "\n",
        "### **11. Armazenamento dos Resultados**\n",
        "```python\n",
        "df_results.to_csv(\"quantum_model_results.csv\", index=False)\n",
        "```\n",
        "- O DataFrame contendo as previsões, resíduos e features é salvo para análises posteriores.\n",
        "\n",
        "---\n",
        "\n",
        "### **Para que Serve**\n",
        "Este código utiliza computação quântica para resolver problemas de aprendizado de máquina. Ele serve para:\n",
        "1. Demonstrar como modelos quânticos podem ser usados para processar dados clássicos.\n",
        "2. Treinar um modelo quântico que aprende a prever rótulos com base nos dados fornecidos.\n",
        "3. Avaliar a performance do modelo por meio de métricas como custo, previsões e resíduos.\n",
        "4. Visualizar os resultados para entender o comportamento do modelo e identificar possíveis ajustes.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ztUtdHcjEZI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "\n",
        "# Configuração do dispositivo quântico\n",
        "n_qubits = 10\n",
        "n_layers = 4  # Aumentar o número de camadas para maior expressividade\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Função de embedding quântico\n",
        "def data_embedding(features, wires):\n",
        "    for i, wire in enumerate(wires):\n",
        "        qml.RY(features[i], wires=wire)\n",
        "\n",
        "# Função do modelo quântico\n",
        "def quantum_model(weights, features):\n",
        "    data_embedding(features, range(n_qubits))\n",
        "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "# QNode com Pennylane\n",
        "weights_shape = (n_layers, n_qubits)\n",
        "\n",
        "@qml.qnode(dev)\n",
        "def circuit(weights, features):\n",
        "    return quantum_model(weights, features)\n",
        "\n",
        "# Função de custo\n",
        "def cost(weights, X, y):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        pred = circuit(weights, X[i])\n",
        "        loss += (pred - y[i])**2\n",
        "    return loss / len(X)\n",
        "\n",
        "# Inicialização de pesos\n",
        "weights = np.random.uniform(low=-0.1, high=0.1, size=weights_shape, requires_grad=True)\n",
        "\n",
        "# Configuração do otimizador\n",
        "opt = AdamOptimizer(stepsize=0.01)\n",
        "steps = 50  # Número de iterações\n",
        "\n",
        "# Dados de treino e teste (ajustar para suportar as dimensões corretas)\n",
        "X_train_resized = X_train[:, :n_qubits]  # Usar apenas os primeiros n_qubits features\n",
        "X_test_resized = X_test[:, :n_qubits]\n",
        "\n",
        "# Treinamento\n",
        "train_costs = []\n",
        "test_costs = []\n",
        "\n",
        "for step in range(steps):\n",
        "    weights = opt.step(lambda w: cost(w, X_train_resized, y_train), weights)\n",
        "    train_cost = cost(weights, X_train_resized, y_train)\n",
        "    test_cost = cost(weights, X_test_resized, y_test)\n",
        "    train_costs.append(train_cost)\n",
        "    test_costs.append(test_cost)\n",
        "    if step % 10 == 0:\n",
        "        print(f\"Step {step}/{steps}: Train Cost = {train_cost:.4f} | Test Cost = {test_cost:.4f}\")\n",
        "\n",
        "# Avaliação final\n",
        "final_train_cost = cost(weights, X_train_resized, y_train)\n",
        "final_test_cost = cost(weights, X_test_resized, y_test)\n",
        "print(f\"Custo final no conjunto de treino: {final_train_cost:.4f}\")\n",
        "print(f\"Custo final no conjunto de teste: {final_test_cost:.4f}\")\n",
        "\n",
        "# Previsões\n",
        "y_train_pred = [float(circuit(weights, x)) for x in X_train_resized]\n",
        "y_test_pred = [float(circuit(weights, x)) for x in X_test_resized]\n",
        "\n",
        "# Criação do DataFrame com resíduos\n",
        "df_results = pd.DataFrame({\n",
        "    \"train_features\": [x.tolist() for x in X_train_resized],\n",
        "    \"train_labels\": y_train,\n",
        "    \"train_predictions\": y_train_pred,\n",
        "    \"train_residuals\": [pred - y for pred, y in zip(y_train_pred, y_train)],\n",
        "    \"test_features\": [x.tolist() for x in X_test_resized],\n",
        "    \"test_labels\": y_test,\n",
        "    \"test_predictions\": y_test_pred,\n",
        "    \"test_residuals\": [pred - y for pred, y in zip(y_test_pred, y_test)]\n",
        "})\n",
        "\n",
        "# Salvar os resultados no DataFrame\n",
        "df_results.to_csv(\"quantum_model_results.csv\", index=False)\n",
        "print(\"Resultados salvos no arquivo 'quantum_model_results.csv'\")\n",
        "\n",
        "# Visualizações\n",
        "\n",
        "# 1. Gráfico de custo durante o treinamento\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(steps), train_costs, label=\"Custo de Treinamento\", color=\"blue\")\n",
        "plt.plot(range(steps), test_costs, label=\"Custo de Teste\", color=\"orange\")\n",
        "plt.title(\"Evolução do Custo Durante o Treinamento\")\n",
        "plt.xlabel(\"Passos\")\n",
        "plt.ylabel(\"Custo\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 2. Gráfico de dispersão (predição vs rótulo)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_train, y_train_pred, alpha=0.7, label=\"Treino\", color=\"blue\")\n",
        "plt.scatter(y_test, y_test_pred, alpha=0.7, label=\"Teste\", color=\"orange\")\n",
        "plt.title(\"Dispersão: Previsão vs Rótulo\")\n",
        "plt.xlabel(\"Rótulo Verdadeiro\")\n",
        "plt.ylabel(\"Previsão\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 3. Histograma de resíduos\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df_results[\"train_residuals\"], bins=20, alpha=0.7, label=\"Treino\", color=\"blue\")\n",
        "plt.hist(df_results[\"test_residuals\"], bins=20, alpha=0.7, label=\"Teste\", color=\"orange\")\n",
        "plt.title(\"Distribuição dos Resíduos\")\n",
        "plt.xlabel(\"Resíduo\")\n",
        "plt.ylabel(\"Frequência\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "otA1OMV__y0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pennylane.optimize import AdamOptimizer\n",
        "\n",
        "# Função de custo\n",
        "def cost(weights, X, y):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        pred = circuit(weights, X[i])\n",
        "        loss += (pred - y[i])**2\n",
        "    return loss / len(X)\n",
        "\n",
        "# Configurando o otimizador\n",
        "opt = AdamOptimizer(stepsize=0.01)\n",
        "steps = 100  # Número de iterações\n",
        "weights = np.random.random(weights_shape)  # Reinicializando pesos\n",
        "\n",
        "# Treinamento\n",
        "for step in range(steps):\n",
        "    weights = opt.step(lambda w: cost(w, X_train[:50, :n_qubits], y_train[:50]), weights)  # Usando um subset para demonstração\n",
        "    if step % 10 == 0:\n",
        "        c = cost(weights, X_train[:50, :n_qubits], y_train[:50])\n",
        "        print(f\"Step {step}: Cost = {c:.4f}\")\n",
        "\n",
        "# Avaliação\n",
        "test_cost = cost(weights, X_test[:50, :n_qubits], y_test[:50])\n",
        "print(f\"Custo no conjunto de teste: {test_cost:.4f}\")\n"
      ],
      "metadata": {
        "id": "22JKgaL78g7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Distribuição dos rótulos de treinamento:\", np.unique(y_train[:50], return_counts=True))\n",
        "print(\"Distribuição dos rótulos de teste:\", np.unique(y_test[:50], return_counts=True))\n"
      ],
      "metadata": {
        "id": "LjjjZYKCAa1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O resultado confirma que o conjunto de treinamento contém apenas a classe \\( 0 \\), e a classe \\( 1 \\) está ausente. Isso é um problema, pois o modelo não pode aprender a diferenciar entre as classes se apenas uma delas estiver presente.\n",
        "\n",
        "### Soluções Possíveis\n",
        "1. **Verificar o Dataset Original**:\n",
        "   - Certifique-se de que o dataset original contém amostras de todas as classes e que os dados foram carregados corretamente.\n",
        "\n",
        "2. **Balancear o Dataset**:\n",
        "   - Se o dataset original for desequilibrado, tente aumentar ou incluir amostras da classe \\( 1 \\) no conjunto de treinamento.\n",
        "\n",
        "3. **Ajustar a Divisão dos Dados**:\n",
        "   - Reavalie a divisão de dados em treinamento e teste para garantir que ambos contenham todas as classes.\n",
        "\n",
        "4. **Código para Checar o Dataset Original**:\n",
        "   Para verificar a distribuição das classes no dataset completo:\n",
        "   ```python\n",
        "   print(\"Distribuição das classes no conjunto completo:\", np.unique(y_train + y_test, return_counts=True))\n",
        "   ```\n",
        "\n",
        "5. **Exemplo de Balanceamento Manual**:\n",
        "   Caso o dataset original tenha classes suficientes, você pode aumentar a classe minoritária:\n",
        "   ```python\n",
        "   # Reamostrando manualmente\n",
        "   if len(class_1_indices) > 0:\n",
        "       min_class_size = min(len(class_0_indices), len(class_1_indices))\n",
        "       balanced_indices = np.hstack((\n",
        "           resample(class_0_indices, n_samples=min_class_size, random_state=42),\n",
        "           resample(class_1_indices, n_samples=min_class_size, random_state=42)\n",
        "       ))\n",
        "\n",
        "       X_train_balanced = X_train[balanced_indices]\n",
        "       y_train_balanced = y_train[balanced_indices]\n",
        "\n",
        "       print(\"Distribuição balanceada:\", np.unique(y_train_balanced, return_counts=True))\n",
        "   else:\n",
        "       print(\"A classe 1 não está presente no conjunto original.\")\n",
        "   ```\n",
        "\n",
        "Se o problema persistir, você pode me informar sobre os detalhes do dataset original para que possamos ajustar o pipeline de pré-processamento."
      ],
      "metadata": {
        "id": "6TjjTiZXEVVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar as classes presentes no conjunto de treinamento\n",
        "print(\"Distribuição original dos rótulos:\", np.unique(y_train, return_counts=True))\n",
        "\n",
        "# Se não houver dados para uma classe, adicione ou corrija a amostragem\n",
        "if len(class_1_indices) == 0:\n",
        "    print(\"A classe 1 não está presente no conjunto de treinamento.\")\n",
        "else:\n",
        "    # Balancear os dados se ambas as classes estiverem presentes\n",
        "    min_class_size = min(len(class_0_indices), len(class_1_indices))\n",
        "    balanced_indices = np.hstack((\n",
        "        resample(class_0_indices, n_samples=min_class_size, random_state=42),\n",
        "        resample(class_1_indices, n_samples=min_class_size, random_state=42)\n",
        "    ))\n",
        "\n",
        "    # Balancear X_train e y_train\n",
        "    X_train_balanced = X_train[balanced_indices]\n",
        "    y_train_balanced = y_train[balanced_indices]\n",
        "\n",
        "    print(\"Distribuição balanceada dos rótulos de treinamento:\", np.unique(y_train_balanced, return_counts=True))\n"
      ],
      "metadata": {
        "id": "ScJ3rD3IBbqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reamostrando manualmente\n",
        "if len(class_1_indices) > 0:\n",
        "    min_class_size = min(len(class_0_indices), len(class_1_indices))\n",
        "    balanced_indices = np.hstack((\n",
        "        resample(class_0_indices, n_samples=min_class_size, random_state=42),\n",
        "        resample(class_1_indices, n_samples=min_class_size, random_state=42)\n",
        "    ))\n",
        "\n",
        "    X_train_balanced = X_train[balanced_indices]\n",
        "    y_train_balanced = y_train[balanced_indices]\n",
        "\n",
        "    print(\"Distribuição balanceada:\", np.unique(y_train_balanced, return_counts=True))\n",
        "else:\n",
        "    print(\"A classe 1 não está presente no conjunto original.\")\n"
      ],
      "metadata": {
        "id": "AWhMOjvvBo0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O resultado indica que a classe \\( 1 \\) está completamente ausente no dataset, não apenas no conjunto de treinamento, mas aparentemente no conjunto original carregado. Isso pode ser causado por:\n",
        "\n",
        "1. **Problemas no Dataset Original**:\n",
        "   - O dataset fornecido contém apenas amostras da classe \\( 0 \\).\n",
        "2. **Erro na Organização do Dataset**:\n",
        "   - Pode haver uma falha no pré-processamento ou na separação das classes.\n",
        "\n",
        "### Próximos Passos\n",
        "\n",
        "#### 1. Verificar o Dataset Original\n",
        "Certifique-se de que o dataset original contém amostras de ambas as classes. Se houver subdiretórios como \"maligno\" e \"benigno\", confirme que ambos foram processados. Use o seguinte código para listar os diretórios e a contagem de arquivos em cada um:\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "# Listar subdiretórios no dataset\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    print(f\"Diretório: {root}, Número de arquivos: {len(files)}\")\n",
        "```\n",
        "\n",
        "#### 2. Corrigir o Pipeline de Pré-processamento\n",
        "Caso o problema esteja na seleção dos dados, ajuste o pipeline para incluir todas as classes. Certifique-se de que o código está capturando ambas as classes:\n",
        "\n",
        "```python\n",
        "class_dirs = [os.path.join(extract_path, \"benigno\"), os.path.join(extract_path, \"maligno\")]\n",
        "\n",
        "# Processar imagens de todas as classes\n",
        "all_image_paths = [img for class_dir in class_dirs for img in glob.glob(f\"{class_dir}/*.jpg\")]\n",
        "processed_images, labels = process_images(all_image_paths, image_size)\n",
        "\n",
        "# Verificar distribuição\n",
        "print(\"Distribuição das classes no dataset completo:\", np.unique(labels, return_counts=True))\n",
        "```\n",
        "\n",
        "#### 3. Adicionar Amostras da Classe Minoritária\n",
        "Se o dataset original é desequilibrado, você pode adicionar manualmente mais amostras da classe \\( 1 \\) (maligno). Isso pode ser feito coletando mais dados ou duplicando algumas amostras existentes para balancear.\n",
        "\n",
        "Se precisar de ajuda para investigar o dataset ou ajustar o pipeline, avise!"
      ],
      "metadata": {
        "id": "HNXMMkQMEIFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Listar subdiretórios no dataset\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    print(f\"Diretório: {root}, Número de arquivos: {len(files)}\")\n"
      ],
      "metadata": {
        "id": "lYLT1udwB0nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_dirs = [os.path.join(extract_path, \"benigno\"), os.path.join(extract_path, \"maligno\")]\n",
        "\n",
        "# Processar imagens de todas as classes\n",
        "all_image_paths = [img for class_dir in class_dirs for img in glob.glob(f\"{class_dir}/*.jpg\")]\n",
        "processed_images, labels = process_images(all_image_paths, image_size)\n",
        "\n",
        "# Verificar distribuição\n",
        "print(\"Distribuição das classes no dataset completo:\", np.unique(labels, return_counts=True))\n"
      ],
      "metadata": {
        "id": "lqaO9eMjB-63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora temos uma distribuição equilibrada no dataset completo:\n",
        "\n",
        "- **500 amostras da classe \"benigno\"**.\n",
        "- **500 amostras da classe \"maligno\"**.\n",
        "\n",
        "Isso confirma que ambas as classes estão presentes e foram corretamente processadas. O problema anterior foi causado por um desequilíbrio ou erro na seleção dos dados para treinamento.\n",
        "\n",
        "### Próximos Passos\n",
        "\n",
        "1. **Codificar os Rótulos**:\n",
        "   - Converter os rótulos `['benigno', 'maligno']` para valores numéricos \\(0\\) e \\(1\\).\n",
        "\n",
        "   ```python\n",
        "   from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "   # Codificar os rótulos\n",
        "   label_encoder = LabelEncoder()\n",
        "   encoded_labels = label_encoder.fit_transform(labels)\n",
        "   print(\"Classes codificadas:\", np.unique(encoded_labels, return_counts=True))\n",
        "   ```\n",
        "\n",
        "2. **Dividir o Dataset**:\n",
        "   - Separar os dados em conjuntos de treinamento e teste de forma balanceada.\n",
        "   \n",
        "   ```python\n",
        "   from sklearn.model_selection import train_test_split\n",
        "\n",
        "   # Dividir os dados\n",
        "   X_train, X_test, y_train, y_test = train_test_split(\n",
        "       processed_images.reshape(len(processed_images), -1),  # Flatten as imagens\n",
        "       encoded_labels,\n",
        "       test_size=0.2,\n",
        "       stratify=encoded_labels,  # Garantir balanceamento\n",
        "       random_state=42\n",
        "   )\n",
        "   print(\"Distribuição dos rótulos no treinamento:\", np.unique(y_train, return_counts=True))\n",
        "   print(\"Distribuição dos rótulos no teste:\", np.unique(y_test, return_counts=True))\n",
        "   ```\n",
        "\n",
        "3. **Treinar o Modelo Quântico**:\n",
        "   - Use os dados balanceados para treinar o modelo quântico e avaliar o desempenho.\n",
        "\n",
        "Deseja que eu implemente essas etapas ou passe direto para o treinamento?"
      ],
      "metadata": {
        "id": "jsWIBtxtDvUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Codificar os rótulos\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "print(\"Classes codificadas:\", np.unique(encoded_labels, return_counts=True))\n"
      ],
      "metadata": {
        "id": "F53QU2yYCJSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dividir os dados\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    processed_images.reshape(len(processed_images), -1),  # Flatten as imagens\n",
        "    encoded_labels,\n",
        "    test_size=0.2,\n",
        "    stratify=encoded_labels,  # Garantir balanceamento\n",
        "    random_state=42\n",
        ")\n",
        "print(\"Distribuição dos rótulos no treinamento:\", np.unique(y_train, return_counts=True))\n",
        "print(\"Distribuição dos rótulos no teste:\", np.unique(y_test, return_counts=True))\n"
      ],
      "metadata": {
        "id": "SbRa_3cJCTO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora o dataset está devidamente balanceado:\n",
        "\n",
        "- **Treinamento**: 400 amostras de cada classe (\\( 0 \\) e \\( 1 \\)).\n",
        "- **Teste**: 100 amostras de cada classe (\\( 0 \\) e \\( 1 \\)).\n",
        "\n",
        "Com os dados preparados, podemos seguir para o treinamento do modelo quântico.\n",
        "\n",
        "### Próximos Passos\n",
        "\n",
        "1. **Treinar o Modelo Quântico**:\n",
        "   - Ajustar os pesos do circuito para minimizar a função de custo.\n",
        "   - Utilizar o conjunto de treinamento balanceado (\\( X\\_train \\) e \\( y\\_train \\)).\n",
        "\n",
        "2. **Avaliar o Modelo**:\n",
        "   - Calcular a precisão e a perda no conjunto de teste (\\( X\\_test \\) e \\( y\\_test \\)).\n",
        "\n",
        "### Código para Treinamento\n",
        "Aqui está o código atualizado para treinar e avaliar o modelo:\n",
        "\n",
        "```python\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "\n",
        "# Função de custo\n",
        "def cost(weights, X, y):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        pred = circuit(weights, X[i])\n",
        "        loss += (pred - y[i])**2\n",
        "    return loss / len(X)\n",
        "\n",
        "# Configurando o otimizador\n",
        "opt = AdamOptimizer(stepsize=0.01)\n",
        "steps = 50  # Número de iterações\n",
        "weights = np.random.random(weights_shape)  # Reinicializando pesos\n",
        "\n",
        "# Treinamento\n",
        "for step in range(steps):\n",
        "    weights = opt.step(lambda w: cost(w, X_train[:, :n_qubits], y_train), weights)\n",
        "    if step % 10 == 0:\n",
        "        c = cost(weights, X_train[:, :n_qubits], y_train)\n",
        "        print(f\"Step {step}: Cost = {c:.4f}\")\n",
        "\n",
        "# Avaliação no conjunto de teste\n",
        "test_cost = cost(weights, X_test[:, :n_qubits], y_test)\n",
        "print(f\"Custo no conjunto de teste: {test_cost:.4f}\")\n",
        "```\n",
        "\n",
        "### Explicação\n",
        "1. **Função de Custo**:\n",
        "   - Calcula o erro quadrático médio entre a previsão do circuito e os rótulos reais.\n",
        "2. **Otimização**:\n",
        "   - Usa Adam para ajustar os pesos do circuito.\n",
        "3. **Avaliação**:\n",
        "   - Mede o custo no conjunto de teste para verificar a generalização.\n",
        "\n",
        "Deseja executar este código ou ajustar algum parâmetro antes de seguir?"
      ],
      "metadata": {
        "id": "JEwGaoJzDhsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "\n",
        "# Função de custo\n",
        "def cost(weights, X, y):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        pred = circuit(weights, X[i])\n",
        "        loss += (pred - y[i])**2\n",
        "    return loss / len(X)\n",
        "\n",
        "# Configurando o otimizador\n",
        "opt = AdamOptimizer(stepsize=0.01)\n",
        "steps = 50  # Número de iterações\n",
        "weights = np.random.random(weights_shape)  # Reinicializando pesos\n",
        "train_costs = []\n",
        "test_costs = []\n",
        "\n",
        "# Configurando o gráfico\n",
        "plt.ion()  # Ativando o modo interativo\n",
        "fig, ax = plt.subplots()\n",
        "line1, = ax.plot([], [], label='Custo de Treinamento', color='blue')\n",
        "line2, = ax.plot([], [], label='Custo de Teste', color='orange')\n",
        "ax.set_xlim(0, steps)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_xlabel(\"Passo\")\n",
        "ax.set_ylabel(\"Custo\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "# Treinamento\n",
        "start_time = time.time()\n",
        "for step in range(steps):\n",
        "    weights = opt.step(lambda w: cost(w, X_train[:, :n_qubits], y_train), weights)\n",
        "    train_cost = cost(weights, X_train[:, :n_qubits], y_train)\n",
        "    test_cost = cost(weights, X_test[:, :n_qubits], y_test)\n",
        "\n",
        "    # Armazenar custos\n",
        "    train_costs.append(train_cost)\n",
        "    test_costs.append(test_cost)\n",
        "\n",
        "    # Atualizar gráfico\n",
        "    line1.set_data(range(step + 1), train_costs)\n",
        "    line2.set_data(range(step + 1), test_costs)\n",
        "    ax.set_ylim(0, max(train_costs + test_costs) * 1.1)  # Ajustar limites do gráfico dinamicamente\n",
        "    clear_output(wait=True)\n",
        "    plt.draw()\n",
        "    plt.pause(0.1)\n",
        "\n",
        "    # Print no console\n",
        "    print(f\"Passo {step}/{steps} | Custo de Treinamento: {train_cost:.4f} | Custo de Teste: {test_cost:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Treinamento concluído em {end_time - start_time:.2f} segundos.\")\n",
        "plt.ioff()  # Desativar o modo interativo\n",
        "plt.show()\n",
        "\n",
        "# Avaliação Final no Conjunto de Teste\n",
        "final_test_cost = cost(weights, X_test[:, :n_qubits], y_test)\n",
        "print(f\"Custo final no conjunto de teste: {final_test_cost:.4f}\")\n"
      ],
      "metadata": {
        "id": "1TylwODwCjzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Previsões no conjunto de teste\n",
        "y_pred = [round(float(circuit(weights, x))) for x in X_test[:, :n_qubits]]\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Acurácia no conjunto de teste: {accuracy:.2%}\")\n"
      ],
      "metadata": {
        "id": "69mk3xhTdn-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Relatório de classificação\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Benigno\", \"Maligno\"]))\n"
      ],
      "metadata": {
        "id": "Flu_IGkneQT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "import numpy as np\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "\n",
        "# Configurando o dispositivo quântico\n",
        "n_qubits = 12  # Número de qubits\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Redefinir o circuito com o dispositivo atualizado\n",
        "@qml.qnode(dev)\n",
        "def circuit(weights, features):\n",
        "    # Embedding das features no circuito usando rotações RY\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(features[i], wires=i)\n",
        "    # Camadas parametrizadas\n",
        "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "    # Medida no primeiro qubit\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "# Parâmetros iniciais para testes\n",
        "weights_shape = (4, n_qubits)  # 4 camadas e 12 qubits\n",
        "weights = np.random.random(weights_shape)  # Pesos aleatórios\n",
        "features = np.random.random(n_qubits)  # Exemplo de entrada\n",
        "\n",
        "# Testando o circuito\n",
        "output = circuit(weights, features)\n",
        "print(f\"Saída do circuito: {output}\")\n"
      ],
      "metadata": {
        "id": "Pmx4JWJLe0lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "import numpy as np\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "\n",
        "# Configurando o dispositivo quântico\n",
        "n_qubits = 12  # Número de qubits\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Redefinir o circuito com o dispositivo atualizado\n",
        "@qml.qnode(dev)\n",
        "def circuit(weights, features):\n",
        "    # Embedding das features no circuito usando rotações RY\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(features[i], wires=i)\n",
        "    # Camadas parametrizadas\n",
        "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "    # Medida no primeiro qubit\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "# Função de custo\n",
        "def cost(weights, X, y):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        pred = circuit(weights, X[i])\n",
        "        loss += (pred - y[i]) ** 2  # Erro quadrático\n",
        "    return loss / len(X)\n",
        "\n",
        "# Configuração do treinamento\n",
        "weights_shape = (4, n_qubits)  # 4 camadas e 12 qubits\n",
        "weights = np.random.random(weights_shape)  # Inicialização dos pesos aleatórios\n",
        "opt = AdamOptimizer(stepsize=0.01)  # Otimizador Adam\n",
        "steps = 50  # Número de iterações\n",
        "\n",
        "# Dados simulados para teste\n",
        "X_train = np.random.random((100, n_qubits))  # 100 amostras, cada uma com 12 qubits\n",
        "y_train = np.random.choice([0, 1], size=100)  # Rótulos binários simulados\n",
        "\n",
        "# Treinamento\n",
        "for step in range(steps):\n",
        "    weights = opt.step(lambda w: cost(w, X_train, y_train), weights)  # Atualizar pesos\n",
        "    if step % 10 == 0:  # Exibir progresso a cada 10 iterações\n",
        "        current_cost = cost(weights, X_train, y_train)\n",
        "        print(f\"Passo {step}/{steps} | Custo: {current_cost:.4f}\")\n",
        "\n",
        "# Resultado final\n",
        "final_cost = cost(weights, X_train, y_train)\n",
        "print(f\"Custo final após {steps} passos: {final_cost:.4f}\")\n"
      ],
      "metadata": {
        "id": "ifCUQ5oBgKWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane.numpy as pnp  # Usar NumPy do PennyLane para suporte a gradientes\n",
        "\n",
        "# Configuração dos pesos ajustada\n",
        "weights = pnp.random.uniform(low=-0.1, high=0.1, size=weights_shape, requires_grad=True)\n",
        "\n",
        "# Ajustar rótulos para intervalo compatível\n",
        "y_train = 2 * y_train - 1  # Converte 0, 1 para -1, 1\n",
        "\n",
        "# Otimizador com taxa de aprendizado maior\n",
        "opt = AdamOptimizer(stepsize=0.1)\n",
        "\n",
        "# Treinamento com ajustes\n",
        "for step in range(steps):\n",
        "    weights = opt.step(lambda w: cost(w, X_train, y_train), weights)\n",
        "    if step % 10 == 0:\n",
        "        current_cost = cost(weights, X_train, y_train)\n",
        "        print(f\"Passo {step}/{steps} | Custo: {current_cost:.4f}\")\n",
        "\n",
        "final_cost = cost(weights, X_train, y_train)\n",
        "print(f\"Custo final após {steps} passos: {final_cost:.4f}\")\n"
      ],
      "metadata": {
        "id": "lq9f9d4ioAvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Previsões com transformação para -1 ou 1\n",
        "y_pred = [1 if float(circuit(weights, x)) >= 0 else -1 for x in X_train]\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(f\"Acurácia no conjunto de treinamento: {accuracy:.2%}\")\n"
      ],
      "metadata": {
        "id": "hH-ZFY7KsgFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Previsões com transformação para -1 ou 1\n",
        "y_pred = [1 if float(circuit(weights, x)) >= 0 else -1 for x in X_train]\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(f\"Acurácia no conjunto de treinamento: {accuracy:.2%}\")\n"
      ],
      "metadata": {
        "id": "FHYzN1xKs14i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está o código ajustado com melhorias para aumentar a expressividade do circuito, regularização na função de custo e validação em dados de teste.\n",
        "\n",
        "---\n",
        "\n",
        "### Código Ajustado\n",
        "\n",
        "```python\n",
        "import pennylane as qml\n",
        "import pennylane.numpy as pnp\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "\n",
        "# Configurando o dispositivo quântico\n",
        "n_qubits = 12  # Número de qubits\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Redefinir o circuito com mais camadas\n",
        "@qml.qnode(dev)\n",
        "def circuit(weights, features):\n",
        "    # Embedding das features no circuito usando rotações RY\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(features[i], wires=i)\n",
        "    # Camadas parametrizadas\n",
        "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "    # Medida no primeiro qubit\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "# Função de custo com regularização L2\n",
        "def cost(weights, X, y):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        pred = circuit(weights, X[i])\n",
        "        loss += (pred - y[i]) ** 2  # Erro quadrático\n",
        "    reg_term = 0.01 * pnp.sum(weights**2)  # Regularização L2\n",
        "    return loss / len(X) + reg_term\n",
        "\n",
        "# Configuração do treinamento\n",
        "n_layers = 6  # Aumentar o número de camadas\n",
        "weights_shape = (n_layers, n_qubits)\n",
        "weights = pnp.random.uniform(low=-0.1, high=0.1, size=weights_shape, requires_grad=True)\n",
        "opt = AdamOptimizer(stepsize=0.1)\n",
        "steps = 100  # Aumentar o número de iterações\n",
        "\n",
        "# Ajustar rótulos para intervalo compatível\n",
        "y_train = 2 * y_train - 1  # Converte 0, 1 para -1, 1\n",
        "y_test = 2 * y_test - 1  # Converte 0, 1 para -1, 1\n",
        "\n",
        "# Treinamento\n",
        "for step in range(steps):\n",
        "    weights = opt.step(lambda w: cost(w, X_train, y_train), weights)\n",
        "    if step % 10 == 0:  # Exibir progresso a cada 10 iterações\n",
        "        current_cost = cost(weights, X_train, y_train)\n",
        "        print(f\"Passo {step}/{steps} | Custo: {current_cost:.4f}\")\n",
        "\n",
        "# Resultado no conjunto de treinamento\n",
        "y_train_pred = [1 if float(circuit(weights, x)) >= 0 else -1 for x in X_train]\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "print(f\"Acurácia no conjunto de treinamento: {train_accuracy:.2%}\")\n",
        "\n",
        "# Avaliação no conjunto de teste\n",
        "y_test_pred = [1 if float(circuit(weights, x)) >= 0 else -1 for x in X_test]\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Acurácia no conjunto de teste: {test_accuracy:.2%}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### O Que Foi Ajustado\n",
        "1. **Camadas Adicionais no Circuito**:\n",
        "   - O número de camadas foi aumentado para 6 para maior expressividade.\n",
        "2. **Regularização L2**:\n",
        "   - Adicionada regularização à função de custo para melhorar a estabilidade do modelo.\n",
        "3. **Mais Iterações**:\n",
        "   - O número de passos foi aumentado para 100 para permitir melhor convergência.\n",
        "4. **Avaliação no Conjunto de Teste**:\n",
        "   - Adicionado código para calcular a acurácia no conjunto de teste.\n",
        "\n",
        "---\n",
        "\n",
        "### Próximos Passos\n",
        "1. **Execute o Código**:\n",
        "   - Observe os custos e as acurácias no conjunto de treinamento e teste.\n",
        "2. **Analise os Resultados**:\n",
        "   - Verifique se há sinais de overfitting (acurácia no treino muito maior que no teste).\n",
        "\n",
        "Se precisar de mais ajustes, estou à disposição! 😊"
      ],
      "metadata": {
        "id": "IS2WKZ4burMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "import pennylane.numpy as pnp\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "\n",
        "# Configurando o dispositivo quântico\n",
        "n_qubits = 12  # Número de qubits\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Redefinir o circuito com mais camadas\n",
        "@qml.qnode(dev)\n",
        "def circuit(weights, features):\n",
        "    # Embedding das features no circuito usando rotações RY\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(features[i], wires=i)\n",
        "    # Camadas parametrizadas\n",
        "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "    # Medida no primeiro qubit\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "# Função de custo com regularização L2\n",
        "def cost(weights, X, y):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        pred = circuit(weights, X[i])\n",
        "        loss += (pred - y[i]) ** 2  # Erro quadrático\n",
        "    reg_term = 0.01 * pnp.sum(weights**2)  # Regularização L2\n",
        "    return loss / len(X) + reg_term\n",
        "\n",
        "# Configuração do treinamento\n",
        "n_layers = 6  # Aumentar o número de camadas\n",
        "weights_shape = (n_layers, n_qubits)\n",
        "weights = pnp.random.uniform(low=-0.1, high=0.1, size=weights_shape, requires_grad=True)\n",
        "opt = AdamOptimizer(stepsize=0.1)\n",
        "steps = 100  # Aumentar o número de iterações\n",
        "\n",
        "# Ajustar rótulos para intervalo compatível\n",
        "y_train = 2 * y_train - 1  # Converte 0, 1 para -1, 1\n",
        "y_test = 2 * y_test - 1  # Converte 0, 1 para -1, 1\n",
        "\n",
        "# Treinamento\n",
        "for step in range(steps):\n",
        "    weights = opt.step(lambda w: cost(w, X_train, y_train), weights)\n",
        "    if step % 10 == 0:  # Exibir progresso a cada 10 iterações\n",
        "        current_cost = cost(weights, X_train, y_train)\n",
        "        print(f\"Passo {step}/{steps} | Custo: {current_cost:.4f}\")\n",
        "\n",
        "# Resultado no conjunto de treinamento\n",
        "y_train_pred = [1 if float(circuit(weights, x)) >= 0 else -1 for x in X_train]\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "print(f\"Acurácia no conjunto de treinamento: {train_accuracy:.2%}\")\n",
        "\n",
        "# Avaliação no conjunto de teste\n",
        "y_test_pred = [1 if float(circuit(weights, x)) >= 0 else -1 for x in X_test]\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Acurácia no conjunto de teste: {test_accuracy:.2%}\")\n"
      ],
      "metadata": {
        "id": "aBdq7hhAtuWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código implementa um pipeline quântico-clássico para classificação binária usando circuitos quânticos e o otimizador Adam para ajustar os parâmetros. Aqui está uma explicação detalhada do processo quântico envolvido:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Dispositivo Quântico**\n",
        "```python\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "```\n",
        "- **Descrição**: Um dispositivo quântico simulado, configurado para usar 12 qubits.\n",
        "- **Papel**: Serve como o \"computador quântico virtual\" onde os circuitos serão executados.\n",
        "- **Simulação Clássica**: O dispositivo `default.qubit` é um simulador baseado em estado vetorial.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Definição do Circuito**\n",
        "```python\n",
        "@qml.qnode(dev)\n",
        "def circuit(weights, features):\n",
        "    # Embedding das features no circuito usando rotações RY\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(features[i], wires=i)\n",
        "    # Camadas parametrizadas\n",
        "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "    # Medida no primeiro qubit\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "```\n",
        "\n",
        "#### a. **Embedding dos Dados**\n",
        "```python\n",
        "for i in range(n_qubits):\n",
        "    qml.RY(features[i], wires=i)\n",
        "```\n",
        "- **O que faz**:\n",
        "  - Os dados clássicos (features) são mapeados para estados quânticos usando rotações \\( RY \\).\n",
        "  - Cada feature é usada para parametrizar uma rotação em torno do eixo \\( Y \\) para o qubit correspondente.\n",
        "- **Papel**:\n",
        "  - Cria uma representação quântica dos dados.\n",
        "\n",
        "#### b. **Camadas Parametrizadas**\n",
        "```python\n",
        "qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "```\n",
        "- **O que faz**:\n",
        "  - Aplica camadas de entrelaçamento entre os qubits, usando parâmetros treináveis (\\( weights \\)).\n",
        "  - Permite que o modelo quântico capture interdependências complexas entre as features.\n",
        "- **Papel**:\n",
        "  - Adiciona expressividade ao circuito, permitindo que ele represente funções mais complexas.\n",
        "\n",
        "#### c. **Medida**\n",
        "```python\n",
        "return qml.expval(qml.PauliZ(0))\n",
        "```\n",
        "- **O que faz**:\n",
        "  - Mede a expectativa do operador \\( Z \\) no primeiro qubit.\n",
        "  - Retorna um valor contínuo no intervalo \\([-1, 1]\\).\n",
        "- **Papel**:\n",
        "  - Converte o estado quântico final em um valor clássico utilizável.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Função de Custo**\n",
        "```python\n",
        "def cost(weights, X, y):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        pred = circuit(weights, X[i])\n",
        "        loss += (pred - y[i]) ** 2  # Erro quadrático\n",
        "    reg_term = 0.01 * pnp.sum(weights**2)  # Regularização L2\n",
        "    return loss / len(X) + reg_term\n",
        "```\n",
        "- **O que faz**:\n",
        "  - Calcula o erro quadrático médio (\\( MSE \\)) entre as previsões do circuito e os rótulos reais.\n",
        "  - Adiciona um termo de regularização L2 para penalizar pesos altos e evitar overfitting.\n",
        "- **Papel**:\n",
        "  - Orienta o treinamento para ajustar os pesos e minimizar a discrepância entre previsões e rótulos.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Treinamento**\n",
        "```python\n",
        "weights = opt.step(lambda w: cost(w, X_train, y_train), weights)\n",
        "```\n",
        "- **O que faz**:\n",
        "  - O otimizador Adam ajusta os pesos do circuito quântico, minimizando a função de custo.\n",
        "- **Papel**:\n",
        "  - Integra o aprendizado quântico ao pipeline clássico, otimizando os parâmetros do circuito.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Classificação Binária**\n",
        "```python\n",
        "y_train_pred = [1 if float(circuit(weights, x)) >= 0 else -1 for x in X_train]\n",
        "```\n",
        "- **O que faz**:\n",
        "  - Converte as previsões contínuas do circuito (\\([-1, 1]\\)) em rótulos binários (\\( -1, 1 \\)) usando uma função de ativação baseada em threshold.\n",
        "- **Papel**:\n",
        "  - Permite que o modelo faça classificações compatíveis com os rótulos ajustados.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Métricas de Desempenho**\n",
        "```python\n",
        "accuracy = accuracy_score(y_train, y_train_pred)\n",
        "```\n",
        "- **O que faz**:\n",
        "  - Calcula a proporção de previsões corretas.\n",
        "- **Papel**:\n",
        "  - Avalia o desempenho do modelo no conjunto de treinamento e teste.\n",
        "\n",
        "---\n",
        "\n",
        "### Resumo do Processo Quântico\n",
        "1. **Embedding**:\n",
        "   - Os dados clássicos são mapeados para estados quânticos usando rotações \\( RY \\).\n",
        "2. **Camadas Parametrizadas**:\n",
        "   - O circuito aprende padrões complexos nos dados ajustando os pesos.\n",
        "3. **Medida**:\n",
        "   - A expectativa do operador \\( Z \\) no primeiro qubit traduz o estado quântico final em um valor clássico.\n",
        "4. **Treinamento**:\n",
        "   - A função de custo e o otimizador ajustam os pesos para melhorar as previsões.\n",
        "5. **Classificação**:\n",
        "   - O valor contínuo retornado pelo circuito é transformado em rótulos binários.\n",
        "\n",
        "Se precisar de mais detalhes ou ajustes no modelo, estou à disposição! 😊"
      ],
      "metadata": {
        "id": "Ic9V71bYvaCY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_RiqRv-vXW1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}