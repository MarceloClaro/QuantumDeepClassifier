{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPSRETnrC92VZO6XA/5wuN1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarceloClaro/QuantumDeepClassifier/blob/main/Rede_de_Tensores_Qu%C3%A2nticos_(FedQTNs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação das bibliotecas necessárias\n",
        "!pip install qiskit\n",
        "!pip install pennylane\n",
        "!pip install tensorflow-quantum\n",
        "!pip install matplotlib\n",
        "!pip install pillow\n"
      ],
      "metadata": {
        "id": "C6blo-TEU3O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from google.colab import drive\n",
        "\n",
        "# Montar o Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir o diretório de saída no Google Drive\n",
        "output_dir = \"/content/drive/My Drive/quantum\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "p17qpoFhU5CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "# Função para solicitar e verificar o caminho do arquivo ZIP\n",
        "def get_zip_path():\n",
        "    zip_path = input(\"Insira o caminho completo do arquivo melanomas.zip no seu Google Drive (ex: /content/drive/My Drive/melanomas.zip): \")\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(\"Arquivo não encontrado! Verifique o caminho e tente novamente.\")\n",
        "        return None\n",
        "    return zip_path\n",
        "\n",
        "# Solicitar o caminho do arquivo ZIP\n",
        "zip_path = get_zip_path()\n",
        "\n",
        "if zip_path:\n",
        "    # Extrair o arquivo ZIP\n",
        "    extract_path = \"/content/melanomas\"\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # Listar os arquivos extraídos\n",
        "    extracted_files = []\n",
        "    for root, dirs, files in os.walk(extract_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):  # Apenas imagens\n",
        "                extracted_files.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"Número total de imagens extraídas: {len(extracted_files)}\")\n",
        "    print(\"Exemplos de arquivos extraídos:\", extracted_files[:5])\n",
        "\n",
        "    # Função para visualizar imagens por classe\n",
        "    def visualize_images(files, n=5):\n",
        "        \"\"\"Visualizar as primeiras N imagens de cada classe\"\"\"\n",
        "        classes = {}\n",
        "        for file in files:\n",
        "            class_name = os.path.basename(os.path.dirname(file))\n",
        "            if class_name not in classes:\n",
        "                classes[class_name] = []\n",
        "            classes[class_name].append(file)\n",
        "\n",
        "        for class_name, images in classes.items():\n",
        "            print(f\"Classe: {class_name} | Total de imagens: {len(images)}\")\n",
        "            plt.figure(figsize=(15, 5))\n",
        "            plt.suptitle(f\"Exemplos da classe: {class_name}\")\n",
        "            for i, img_path in enumerate(images[:n]):\n",
        "                img = Image.open(img_path)\n",
        "                plt.subplot(1, n, i + 1)\n",
        "                plt.imshow(img)\n",
        "                plt.axis(\"off\")\n",
        "                plt.title(f\"{class_name} {i+1}\")\n",
        "            plt.show()\n",
        "\n",
        "    # Visualizar as imagens\n",
        "    visualize_images(extracted_files, n=5)\n",
        "\n",
        "    # Função para redimensionar e salvar imagens\n",
        "    def resize_and_save_images(files, image_size=(64, 64), save_dir=output_dir):\n",
        "        \"\"\"Redimensiona e salva as imagens no Google Drive organizadas por classe\"\"\"\n",
        "        for file in files:\n",
        "            class_name = os.path.basename(os.path.dirname(file))\n",
        "            class_dir = os.path.join(save_dir, class_name)\n",
        "            os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "            img = Image.open(file).convert('RGB')  # Garantir que a imagem tenha 3 canais\n",
        "            img_resized = img.resize(image_size)\n",
        "            save_path = os.path.join(class_dir, os.path.basename(file))\n",
        "            img_resized.save(save_path)\n",
        "\n",
        "    # Redimensionar e salvar as imagens\n",
        "    resize_and_save_images(extracted_files, image_size=(64, 64), save_dir=output_dir)\n",
        "    print(f\"Imagens redimensionadas e salvas em: {output_dir}\")\n",
        "\n",
        "    # Função para criar um DataFrame com os dados\n",
        "    def create_dataframe(save_dir=output_dir):\n",
        "        \"\"\"Cria um DataFrame com os dados das imagens e seus rótulos\"\"\"\n",
        "        data = []\n",
        "        labels = []\n",
        "        for class_name in os.listdir(save_dir):\n",
        "            class_path = os.path.join(save_dir, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                for img_file in os.listdir(class_path):\n",
        "                    img_path = os.path.join(class_path, img_file)\n",
        "                    img = Image.open(img_path).convert('RGB')\n",
        "                    img_array = np.array(img).flatten() / 255.0  # Normalizar\n",
        "                    data.append(img_array)\n",
        "                    labels.append(class_name)\n",
        "        df = pd.DataFrame(data)\n",
        "        df['label'] = labels\n",
        "        return df\n",
        "\n",
        "    # Criar o DataFrame\n",
        "    df = create_dataframe(save_dir=output_dir)\n",
        "    print(\"Amostra do DataFrame:\")\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(\"Encerrando o processo devido à falta do arquivo ZIP.\")\n"
      ],
      "metadata": {
        "id": "MqiX0WmqU_ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if zip_path:\n",
        "    # Separar features e labels\n",
        "    X = df.drop('label', axis=1).values\n",
        "    y = df['label'].values\n",
        "\n",
        "    # Codificar os rótulos\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "    # Dividir os dados em treino e teste\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "\n",
        "    print(f\"Dados divididos em {X_train.shape[0]} amostras de treino e {X_test.shape[0]} amostras de teste.\")\n",
        "else:\n",
        "    print(\"Não foi possível criar os DataFrames devido à falta do arquivo ZIP.\")\n"
      ],
      "metadata": {
        "id": "CJPa-RsfVJzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Importação das Bibliotecas Necessárias\n",
        "import pennylane as qml\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "from pennylane import numpy as np  # Importar o NumPy do PennyLane\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from google.colab import drive\n",
        "\n",
        "# 2. Montagem do Google Drive e Configuração do Ambiente\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir o diretório de saída no Google Drive\n",
        "output_dir = \"/content/drive/My Drive/quantum\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 3. Definição do Dispositivo Quântico\n",
        "n_qubits = 10\n",
        "n_layers = 8  # Camadas do modelo quântico\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# 4. Função de Embedding Quântico com Múltiplas Rotações\n",
        "def data_embedding(features, wires):\n",
        "    for i, wire in enumerate(wires):\n",
        "        qml.RX(features[i], wires=wire)\n",
        "        qml.RY(features[i], wires=wire)\n",
        "        qml.RZ(features[i], wires=wire)\n",
        "\n",
        "# 5. Modelo Quântico com StronglyEntanglingLayers\n",
        "def quantum_model(weights, features):\n",
        "    data_embedding(features, range(n_qubits))\n",
        "    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "# 6. Definição do QNode\n",
        "@qml.qnode(dev)\n",
        "def circuit(weights, features):\n",
        "    return quantum_model(weights, features)\n",
        "\n",
        "# 7. Função de Custo com Regularização L2\n",
        "def cost_with_regularization(weights, X, y, lambda_reg=0.05):\n",
        "    loss = 0\n",
        "    for i in range(len(X)):\n",
        "        pred = circuit(weights, X[i])\n",
        "        # Transformar a saída do PauliZ de [-1, 1] para [0, 1]\n",
        "        pred_transformed = (pred + 1) / 2\n",
        "        loss += (pred_transformed - y[i])**2\n",
        "    reg_term = lambda_reg * np.sum(weights**2)\n",
        "    return loss / len(X) + reg_term\n",
        "\n",
        "# 8. Inicialização dos Pesos\n",
        "weights_shape = (n_layers, n_qubits, 3)  # StronglyEntanglingLayers requer 3 parâmetros por qubit\n",
        "weights = np.random.uniform(low=-0.1, high=0.1, size=weights_shape, requires_grad=True)\n",
        "\n",
        "# 9. Configuração do Otimizador\n",
        "opt = AdamOptimizer(stepsize=0.015)\n",
        "steps = 100  # Número de iterações\n",
        "early_stopping_patience = 3  # Paciência para Early Stopping\n",
        "min_delta = 1e-4  # Melhoria mínima para considerar\n",
        "\n",
        "# 10. Treinamento do Modelo Quântico\n",
        "best_weights = None\n",
        "best_test_cost = float('inf')\n",
        "no_improvement_count = 0\n",
        "train_costs = []\n",
        "test_costs = []\n",
        "\n",
        "for step in range(steps):\n",
        "    weights = opt.step(lambda w: cost_with_regularization(w, X_train, y_train), weights)\n",
        "    train_cost = cost_with_regularization(weights, X_train, y_train)\n",
        "    test_cost = cost_with_regularization(weights, X_test, y_test)\n",
        "\n",
        "    train_costs.append(train_cost)\n",
        "    test_costs.append(test_cost)\n",
        "\n",
        "    # Early Stopping\n",
        "    if test_cost < best_test_cost - min_delta:\n",
        "        best_test_cost = test_cost\n",
        "        best_weights = weights\n",
        "        no_improvement_count = 0\n",
        "    else:\n",
        "        no_improvement_count += 1\n",
        "\n",
        "    if no_improvement_count >= early_stopping_patience:\n",
        "        print(f\"Parada antecipada no passo {step}. Melhor custo no teste: {best_test_cost:.4f}\")\n",
        "        break\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        print(f\"Step {step}/{steps}: Train Cost = {train_cost:.4f} | Test Cost = {test_cost:.4f}\")\n",
        "\n",
        "# 11. Usar os Melhores Pesos Encontrados\n",
        "weights = best_weights\n",
        "\n",
        "# 12. Avaliação Final\n",
        "final_train_cost = cost_with_regularization(weights, X_train, y_train)\n",
        "final_test_cost = cost_with_regularization(weights, X_test, y_test)\n",
        "print(f\"Custo final no conjunto de treino: {final_train_cost:.4f}\")\n",
        "print(f\"Custo final no conjunto de teste: {final_test_cost:.4f}\")\n",
        "\n",
        "# 13. Previsões\n",
        "y_train_pred_probs = [(circuit(weights, x) + 1) / 2 for x in X_train]  # Probabilidades\n",
        "y_test_pred_probs = [(circuit(weights, x) + 1) / 2 for x in X_test]\n",
        "\n",
        "# Converter probabilidades para classes binárias com limiar 0.5\n",
        "y_train_pred = [1 if prob > 0.5 else 0 for prob in y_train_pred_probs]\n",
        "y_test_pred = [1 if prob > 0.5 else 0 for prob in y_test_pred_probs]\n",
        "\n",
        "# 14. Relatório de Classificação\n",
        "print(\"\\nRelatório de Classificação (Treino):\")\n",
        "print(classification_report(y_train, y_train_pred, target_names=label_encoder.classes_, zero_division=1))\n",
        "print(\"\\nRelatório de Classificação (Teste):\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_, zero_division=1))\n",
        "\n",
        "# 15. Métrica AUC-ROC\n",
        "try:\n",
        "    auc_train = roc_auc_score(y_train, y_train_pred_probs)\n",
        "    auc_test = roc_auc_score(y_test, y_test_pred_probs)\n",
        "    print(f\"\\nAUC-ROC (Treino): {auc_train:.4f}\")\n",
        "    print(f\"AUC-ROC (Teste): {auc_test:.4f}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Erro no cálculo do AUC-ROC: {e}\")\n",
        "\n",
        "# 16. Matriz de Confusão\n",
        "print(\"\\nMatriz de Confusão (Teste):\")\n",
        "print(confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "# 17. Criação de DataFrames Separados para Treino e Teste\n",
        "train_features = [x.tolist() for x in X_train]\n",
        "test_features = [x.tolist() for x in X_test]\n",
        "\n",
        "train_df = pd.DataFrame({\n",
        "    \"features\": train_features,\n",
        "    \"labels\": y_train,\n",
        "    \"predictions\": y_train_pred,\n",
        "    \"predictions_probs\": y_train_pred_probs,\n",
        "    \"residuals\": [y - p for y, p in zip(y_train, y_train_pred)]\n",
        "})\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    \"features\": test_features,\n",
        "    \"labels\": y_test,\n",
        "    \"predictions\": y_test_pred,\n",
        "    \"predictions_probs\": y_test_pred_probs,\n",
        "    \"residuals\": [y - p for y, p in zip(y_test, y_test_pred)]\n",
        "})\n",
        "\n",
        "# 18. Salvando os Resultados no Google Drive\n",
        "train_results_path = os.path.join(output_dir, \"quantum_train_results.csv\")\n",
        "test_results_path = os.path.join(output_dir, \"quantum_test_results.csv\")\n",
        "\n",
        "train_df.to_csv(train_results_path, index=False)\n",
        "test_df.to_csv(test_results_path, index=False)\n",
        "\n",
        "print(f\"\\nResultados de treino salvos em: {train_results_path}\")\n",
        "print(f\"Resultados de teste salvos em: {test_results_path}\")\n",
        "\n",
        "# 19. Visualizações\n",
        "\n",
        "# 1. Gráfico de Custo Durante o Treinamento\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(train_costs)), train_costs, label=\"Custo de Treinamento\", color=\"blue\")\n",
        "plt.plot(range(len(test_costs)), test_costs, label=\"Custo de Teste\", color=\"orange\")\n",
        "plt.title(\"Evolução do Custo Durante o Treinamento\")\n",
        "plt.xlabel(\"Passos\")\n",
        "plt.ylabel(\"Custo\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(output_dir, \"training_costs.png\"))\n",
        "plt.show()\n",
        "\n",
        "# 2. Gráfico de Dispersão (Previsão Probabilística vs Rótulo)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_train, y_train_pred_probs, alpha=0.7, label=\"Treino\", color=\"blue\")\n",
        "plt.scatter(y_test, y_test_pred_probs, alpha=0.7, label=\"Teste\", color=\"orange\")\n",
        "plt.title(\"Dispersão: Previsão Probabilística vs Rótulo\")\n",
        "plt.xlabel(\"Rótulo Verdadeiro\")\n",
        "plt.ylabel(\"Probabilidade da Classe 1\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(output_dir, \"scatter_predictions.png\"))\n",
        "plt.show()\n",
        "\n",
        "# 3. Histograma de Resíduos\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(train_df[\"residuals\"], bins=20, alpha=0.7, label=\"Treino\", color=\"blue\")\n",
        "plt.hist(test_df[\"residuals\"], bins=20, alpha=0.7, label=\"Teste\", color=\"orange\")\n",
        "plt.title(\"Distribuição dos Resíduos\")\n",
        "plt.xlabel(\"Resíduo (Real - Predito)\")\n",
        "plt.ylabel(\"Frequência\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(output_dir, \"residuals_histogram.png\"))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fXWTk7VNVNBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Redes de Tensores Quânticos Federados (FedQTNs) para Diagnóstico Médico\n",
        "com Comparação de Modelo Clássico (Exemplo Simplificado)\n",
        "\n",
        "1. Verificação de dados e organização\n",
        "2. Data Augmentation e Redimensionamento de imagens\n",
        "3. Pré-processamento (PCA, UMAP, t-SNE, Balanceamento de classes)\n",
        "4. Aprendizado Federado (Modelo Quântico e Modelo Clássico)\n",
        "5. Grad-CAM simplificado via tf-keras-vis para o modelo clássico (CNN)\n",
        "6. Logging e um teste unitário de exemplo\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Instalação das Bibliotecas Necessárias (comente se já estiverem instaladas)\n",
        "# =============================================================================\n",
        "\n",
        "!pip install qiskit\n",
        "!pip install pennylane\n",
        "!pip install matplotlib\n",
        "!pip install pillow\n",
        "!pip install scikit-learn\n",
        "!pip install albumentations\n",
        "!pip install tensorflow\n",
        "!pip install tf-keras-vis\n",
        "!pip install Augmentor\n",
        "!pip install umap-learn\n",
        "!pip install diffprivlib\n",
        "!pip install pytest\n",
        "\n",
        "# =============================================================================\n",
        "# 2. Importação das Bibliotecas Necessárias\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import umap\n",
        "import albumentations as A\n",
        "import Augmentor\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane.optimize import AdamOptimizer\n",
        "\n",
        "# Importar 'Gaussian' do diffprivlib\n",
        "from diffprivlib.mechanisms import Gaussian\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tf_keras_vis.gradcam import Gradcam\n",
        "from tf_keras_vis.utils.scores import CategoricalScore\n",
        "\n",
        "import pytest  # Para testes unitários\n",
        "\n",
        "# =============================================================================\n",
        "# Configuração de Logging para Monitoramento\n",
        "# =============================================================================\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# =============================================================================\n",
        "# 3. Montagem do Google Drive e Configuração de Diretórios\n",
        "# =============================================================================\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = \"/content/drive/My Drive/quantum_fedQTN\"\n",
        "melanomas_dir = os.path.join(base_dir, \"MELANOMAS\")\n",
        "update_dir = os.path.join(base_dir, \"ATUALIZACAO\")\n",
        "data_dir = os.path.join(base_dir, \"DADOS\")\n",
        "os.makedirs(melanomas_dir, exist_ok=True)\n",
        "os.makedirs(update_dir, exist_ok=True)\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# =============================================================================\n",
        "# 4. Função para Solicitar e Verificar o Caminho do Arquivo ZIP\n",
        "# =============================================================================\n",
        "\n",
        "def get_zip_path():\n",
        "    \"\"\"Solicita ao usuário o caminho completo do arquivo melanomas.zip no seu Google Drive.\"\"\"\n",
        "    zip_path = input(\n",
        "        \"Insira o caminho completo do arquivo melanomas.zip no seu Google Drive: \"\n",
        "    )\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(\"Arquivo não encontrado! Verifique o caminho e tente novamente.\")\n",
        "        return None\n",
        "    return zip_path\n",
        "\n",
        "# =============================================================================\n",
        "# 5. Solicitar o Caminho do Arquivo ZIP\n",
        "# =============================================================================\n",
        "\n",
        "zip_path = get_zip_path()\n",
        "\n",
        "# =============================================================================\n",
        "# 6. Verificação de Organização dos Dados\n",
        "# =============================================================================\n",
        "\n",
        "def verify_data_organization(directory):\n",
        "    \"\"\"\n",
        "    Verifica se os dados estão organizados em subpastas por classe dentro do 'directory'.\n",
        "    Cada subpasta deve conter imagens (.jpg, .png, etc.).\n",
        "    \"\"\"\n",
        "    classes = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
        "    if not classes:\n",
        "        logging.error(f\"Nenhuma subpasta encontrada em {directory}. Verifique a organização dos dados.\")\n",
        "        return False\n",
        "    for cls in classes:\n",
        "        cls_dir = os.path.join(directory, cls)\n",
        "        images = [f for f in os.listdir(cls_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "        if not images:\n",
        "            logging.error(f\"Nenhuma imagem encontrada na subpasta {cls_dir}.\")\n",
        "            return False\n",
        "    logging.info(\"Verificação de organização de dados concluída com sucesso.\")\n",
        "    return True\n",
        "\n",
        "# =============================================================================\n",
        "# 7. Funções Auxiliares para Visualização e Data Augmentation\n",
        "# =============================================================================\n",
        "\n",
        "def visualize_images(files, n=5):\n",
        "    \"\"\"Visualiza as primeiras N imagens de cada classe para monitorar o dataset.\"\"\"\n",
        "    classes = {}\n",
        "    for file in files:\n",
        "        class_name = os.path.basename(os.path.dirname(file))\n",
        "        if class_name not in classes:\n",
        "            classes[class_name] = []\n",
        "        classes[class_name].append(file)\n",
        "\n",
        "    for class_name, images in classes.items():\n",
        "        logging.info(f\"Classe: {class_name} | Total de imagens: {len(images)}\")\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.suptitle(f\"Exemplos da classe: {class_name}\")\n",
        "        for i, img_path in enumerate(images[:n]):\n",
        "            img = Image.open(img_path)\n",
        "            plt.subplot(1, n, i + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"{class_name} {i+1}\")\n",
        "        plt.show()\n",
        "\n",
        "def augment_images(files, save_dir):\n",
        "    \"\"\"\n",
        "    Aplica transformações de Data Augmentation usando Albumentations e salva as imagens.\n",
        "    (Removendo ElasticTransform e IAAPiecewiseAffine)\n",
        "    \"\"\"\n",
        "    albumentations_transform = A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.2),\n",
        "        A.Rotate(limit=45, p=0.7),\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.RandomResizedCrop(height=64, width=64, scale=(0.8, 1.0), ratio=(0.9, 1.1), p=0.5),\n",
        "        A.Blur(blur_limit=3, p=0.3),\n",
        "        A.HueSaturationValue(p=0.3),\n",
        "        A.RandomGamma(p=0.3)\n",
        "    ])\n",
        "\n",
        "    for file in files:\n",
        "        img = Image.open(file).convert('RGB')\n",
        "        img_array = np.array(img)\n",
        "        # Gerar 2 imagens com Albumentations\n",
        "        for idx in range(2):\n",
        "            aug_img = albumentations_transform(image=img_array)['image']\n",
        "            aug_img_pil = Image.fromarray(aug_img)\n",
        "            aug_filename = os.path.splitext(os.path.basename(file))[0] + f\"_alb_aug{idx}.jpg\"\n",
        "            aug_img_pil.save(os.path.join(save_dir, aug_filename))\n",
        "            logging.info(f\"Imagem augmentada salva em: {os.path.join(save_dir, aug_filename)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 8. Função para Redimensionar e Salvar Imagens\n",
        "# =============================================================================\n",
        "\n",
        "def resize_and_save_images(files, image_size=(64, 64), save_dir=None):\n",
        "    if save_dir is None:\n",
        "        save_dir = data_dir\n",
        "\n",
        "    for file in files:\n",
        "        class_name = os.path.basename(os.path.dirname(file))\n",
        "        class_dir = os.path.join(save_dir, class_name)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "        img = Image.open(file).convert('RGB')\n",
        "        img_resized = img.resize(image_size)\n",
        "        save_path = os.path.join(class_dir, os.path.basename(file))\n",
        "        img_resized.save(save_path)\n",
        "        logging.info(f\"Imagem redimensionada salva em: {save_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 9. Função para Criar um DataFrame com os Dados\n",
        "# =============================================================================\n",
        "\n",
        "def create_dataframe(save_dir=None):\n",
        "    if save_dir is None:\n",
        "        save_dir = data_dir\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    for class_name in os.listdir(save_dir):\n",
        "        class_path = os.path.join(save_dir, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            for img_file in os.listdir(class_path):\n",
        "                if img_file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                    img_path = os.path.join(class_path, img_file)\n",
        "                    img = Image.open(img_path).convert('RGB')\n",
        "                    img_array = np.array(img).flatten() / 255.0\n",
        "                    data.append(img_array)\n",
        "                    labels.append(class_name)\n",
        "    df = pd.DataFrame(data)\n",
        "    df['label'] = labels\n",
        "    return df\n",
        "\n",
        "# =============================================================================\n",
        "# 10. Carregamento e Processamento do ZIP se Disponível\n",
        "# =============================================================================\n",
        "\n",
        "if zip_path:\n",
        "    # Extrair o Arquivo ZIP\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(melanomas_dir)\n",
        "    logging.info(\"Arquivo ZIP extraído com sucesso.\")\n",
        "\n",
        "    # Verificar a organização dos dados\n",
        "    if not verify_data_organization(melanomas_dir):\n",
        "        raise Exception(\"Organização de dados incorreta. Verifique o diretório de imagens.\")\n",
        "\n",
        "    # Listar arquivos extraídos\n",
        "    extracted_files = []\n",
        "    for root, dirs, files in os.walk(melanomas_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                extracted_files.append(os.path.join(root, file))\n",
        "\n",
        "    logging.info(f\"Número total de imagens extraídas: {len(extracted_files)}\")\n",
        "    logging.info(f\"Exemplos de arquivos extraídos: {extracted_files[:5]}\")\n",
        "\n",
        "    # Visualizar algumas imagens\n",
        "    visualize_images(extracted_files, n=5)\n",
        "\n",
        "    # Aplicar Data Augmentation\n",
        "    augment_images(extracted_files, save_dir=melanomas_dir)\n",
        "    logging.info(\"Data Augmentation concluída.\")\n",
        "\n",
        "    # Atualizar lista de imagens após augmentação\n",
        "    augmented_files = []\n",
        "    for root, dirs, files in os.walk(melanomas_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                augmented_files.append(os.path.join(root, file))\n",
        "\n",
        "    logging.info(f\"Número total de imagens após Data Augmentation: {len(augmented_files)}\")\n",
        "\n",
        "    # Redimensionar e salvar as imagens\n",
        "    resize_and_save_images(augmented_files, image_size=(64, 64), save_dir=data_dir)\n",
        "    logging.info(f\"Imagens redimensionadas e salvas em: {data_dir}\")\n",
        "\n",
        "    # Criar DataFrame final\n",
        "    df = create_dataframe(save_dir=data_dir)\n",
        "    logging.info(\"Amostra do DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # =============================================================================\n",
        "    # 11. Pré-processamento (PCA, UMAP, t-SNE, Balanceamento)\n",
        "    # =============================================================================\n",
        "\n",
        "    X = df.drop('label', axis=1).values\n",
        "    y = df['label'].values\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "    def apply_pca(X, variance_threshold=0.95):\n",
        "        pca = PCA(n_components=variance_threshold, svd_solver='full')\n",
        "        X_pca = pca.fit_transform(X)\n",
        "        logging.info(f\"PCA -> {X_pca.shape[1]} comps explicando {pca.explained_variance_ratio_.sum()*100:.2f}% da variância.\")\n",
        "        return X_pca, pca\n",
        "\n",
        "    variance_thresholds = [0.90, 0.95, 0.99]\n",
        "    pca_results = {}\n",
        "    for threshold in variance_thresholds:\n",
        "        logging.info(f\"\\nAplicando PCA c/ threshold de variância: {threshold}\")\n",
        "        X_pca_tmp, pca_obj = apply_pca(X, variance_threshold=threshold)\n",
        "        pca_results[threshold] = (X_pca_tmp, pca_obj)\n",
        "\n",
        "    chosen_threshold = 0.95\n",
        "    X_pca, pca = pca_results[chosen_threshold]\n",
        "\n",
        "    def plot_pca_components(pca, num_components=10):\n",
        "        plt.figure(figsize=(10,6))\n",
        "        plt.bar(range(1, num_components+1), pca.explained_variance_ratio_[:num_components]*100)\n",
        "        plt.xlabel('Componentes Principais')\n",
        "        plt.ylabel('Variância Explicada (%)')\n",
        "        plt.title('Importância dos Componentes Principais no PCA')\n",
        "        plt.show()\n",
        "\n",
        "    plot_pca_components(pca, num_components=10)\n",
        "\n",
        "    def apply_umap_tsne(X, n_components=2):\n",
        "        umap_model = umap.UMAP(n_components=n_components, random_state=42)\n",
        "        X_umap = umap_model.fit_transform(X)\n",
        "        tsne_model = TSNE(n_components=n_components, random_state=42)\n",
        "        X_tsne = tsne_model.fit_transform(X)\n",
        "        return X_umap, X_tsne\n",
        "\n",
        "    X_umap, X_tsne = apply_umap_tsne(X_pca)\n",
        "\n",
        "    def plot_umap(X_umap, y_encoded, label_encoder):\n",
        "        plt.figure(figsize=(10,6))\n",
        "        for class_idx in np.unique(y_encoded):\n",
        "            plt.scatter(X_umap[y_encoded == class_idx, 0],\n",
        "                        X_umap[y_encoded == class_idx, 1],\n",
        "                        label=label_encoder.inverse_transform([class_idx])[0],\n",
        "                        alpha=0.5)\n",
        "        plt.title('Projeção UMAP dos Dados')\n",
        "        plt.xlabel('UMAP 1')\n",
        "        plt.ylabel('UMAP 2')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_tsne(X_tsne, y_encoded, label_encoder):\n",
        "        plt.figure(figsize=(10,6))\n",
        "        for class_idx in np.unique(y_encoded):\n",
        "            plt.scatter(X_tsne[y_encoded == class_idx, 0],\n",
        "                        X_tsne[y_encoded == class_idx, 1],\n",
        "                        label=label_encoder.inverse_transform([class_idx])[0],\n",
        "                        alpha=0.5)\n",
        "        plt.title('Projeção t-SNE dos Dados')\n",
        "        plt.xlabel('t-SNE 1')\n",
        "        plt.ylabel('t-SNE 2')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    plot_umap(X_umap, y_encoded, label_encoder)\n",
        "    plot_tsne(X_tsne, y_encoded, label_encoder)\n",
        "\n",
        "    df_pca = pd.DataFrame(X_pca)\n",
        "    df_pca['label'] = y_encoded\n",
        "    class_counts = df_pca['label'].value_counts()\n",
        "    min_count = class_counts.min()\n",
        "\n",
        "    df_balanced = pd.DataFrame()\n",
        "    for cls_id in class_counts.index:\n",
        "        df_cls = df_pca[df_pca['label'] == cls_id]\n",
        "        df_cls_resampled = resample(df_cls, replace=True, n_samples=min_count, random_state=42)\n",
        "        df_balanced = pd.concat([df_balanced, df_cls_resampled])\n",
        "\n",
        "    X_balanced = df_balanced.drop('label', axis=1).values\n",
        "    y_balanced = df_balanced['label'].values\n",
        "\n",
        "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "        X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n",
        "    )\n",
        "    logging.info(f\"Dados de treino: {X_train_full.shape[0]}, teste: {X_test.shape[0]}.\")\n",
        "\n",
        "    num_clients = 4\n",
        "    client_data = []\n",
        "    X_split = np.array_split(X_train_full, num_clients)\n",
        "    y_split = np.array_split(y_train_full, num_clients)\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        client_data.append((X_split[i], y_split[i]))\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "            client_data[i][0], client_data[i][1], test_size=0.1,\n",
        "            random_state=42, stratify=client_data[i][1]\n",
        "        )\n",
        "        client_data[i] = (X_tr, y_tr, X_val, y_val)\n",
        "\n",
        "    logging.info(f\"Cliente 1 -> Treino: {client_data[0][0].shape[0]}, Val: {client_data[0][2].shape[0]}\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # 12. Definição do Modelo Quântico (MERA) e Treinamento Federado\n",
        "    # =============================================================================\n",
        "\n",
        "    def MERA(weights, features, quantum_gates='RY'):\n",
        "        wires = list(range(len(features)))\n",
        "        for layer in range(len(weights)):\n",
        "            for i in range(0, len(wires), 2):\n",
        "                if i+1 < len(wires):\n",
        "                    if quantum_gates == 'RY':\n",
        "                        qml.RY(weights[layer][i][0], wires=wires[i])\n",
        "                        qml.RY(weights[layer][i+1][0], wires=wires[i+1])\n",
        "                    elif quantum_gates == 'RX':\n",
        "                        qml.RX(weights[layer][i][0], wires=wires[i])\n",
        "                        qml.RX(weights[layer][i+1][0], wires=wires[i+1])\n",
        "                    qml.CNOT(wires=[wires[i], wires[i+1]])\n",
        "                    if quantum_gates == 'RY':\n",
        "                        qml.RY(weights[layer][i][1], wires=wires[i])\n",
        "                        qml.RY(weights[layer][i+1][1], wires=wires[i+1])\n",
        "                    elif quantum_gates == 'RX':\n",
        "                        qml.RX(weights[layer][i][1], wires=wires[i])\n",
        "                        qml.RX(weights[layer][i+1][1], wires=wires[i+1])\n",
        "            new_wires = []\n",
        "            for j in range(0, len(wires), 2):\n",
        "                if j+1 < len(wires):\n",
        "                    new_wires.append(j//2)\n",
        "            wires = new_wires\n",
        "        return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "    n_qubits = 8\n",
        "    n_layers = 3\n",
        "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "    @qml.qnode(dev)\n",
        "    def circuit_mera(weights, features, quantum_gates='RY'):\n",
        "        for i in range(n_qubits):\n",
        "            qml.RX(features[i], wires=i)\n",
        "            qml.RY(features[i], wires=i)\n",
        "            qml.RZ(features[i], wires=i)\n",
        "        return MERA(weights, features, quantum_gates)\n",
        "\n",
        "    def cost_with_regularization(weights, X, y, lambda_reg=0.05, quantum_gates='RY'):\n",
        "        loss = 0\n",
        "        for i in range(len(X)):\n",
        "            pred = circuit_mera(weights, X[i], quantum_gates)\n",
        "            pred_transformed = (pred + 1)/2\n",
        "            loss += (pred_transformed - y[i])**2\n",
        "        reg_term = lambda_reg * np.sum(weights**2)\n",
        "        return loss / len(X) + reg_term\n",
        "\n",
        "    weights_shape = (n_layers, n_qubits, 2)\n",
        "    weights = np.random.uniform(low=-0.1, high=0.1, size=weights_shape, requires_grad=True)\n",
        "\n",
        "    def federated_averaging(global_weights, client_weights):\n",
        "        avg_weights = copy.deepcopy(global_weights)\n",
        "        for layer in range(len(global_weights)):\n",
        "            for qubit in range(len(global_weights[layer])):\n",
        "                avg_weights[layer][qubit] = np.mean(\n",
        "                    [client_weights[c][layer][qubit] for c in range(len(client_weights))],\n",
        "                    axis=0\n",
        "                )\n",
        "        return avg_weights\n",
        "\n",
        "    opt = AdamOptimizer(stepsize=0.01)\n",
        "    rounds = 5\n",
        "    local_steps = 20\n",
        "    lambda_reg = 0.05\n",
        "    dp_noise = 0.05\n",
        "\n",
        "    def train_federated(global_weights, client_data, rounds=5, local_steps=20,\n",
        "                        lambda_reg=0.05, dp_noise=0.05, quantum_gates='RY'):\n",
        "        privacy_metrics = {}\n",
        "        for r in range(rounds):\n",
        "            logging.info(f\"\\n--- Rodada Federada {r+1}/{rounds} ---\")\n",
        "            client_updates = []\n",
        "            epsilons = []\n",
        "            deltas = []\n",
        "            for c in range(len(client_data)):\n",
        "                logging.info(f\"Treinando Cliente {c+1} (Quântico)\")\n",
        "                local_weights = copy.deepcopy(global_weights)\n",
        "                X_train_q, y_train_q, X_val_q, y_val_q = client_data[c]\n",
        "\n",
        "                best_local_weights = None\n",
        "                best_val_cost = float('inf')\n",
        "                no_improvement = 0\n",
        "                for step in range(local_steps):\n",
        "                    local_weights = opt.step(\n",
        "                        lambda w: cost_with_regularization(w, X_train_q, y_train_q, lambda_reg, quantum_gates),\n",
        "                        local_weights\n",
        "                    )\n",
        "                    val_cost = cost_with_regularization(local_weights, X_val_q, y_val_q, lambda_reg, quantum_gates)\n",
        "                    if val_cost < best_val_cost - 1e-4:\n",
        "                        best_val_cost = val_cost\n",
        "                        best_local_weights = copy.deepcopy(local_weights)\n",
        "                        no_improvement = 0\n",
        "                    else:\n",
        "                        no_improvement += 1\n",
        "                    if no_improvement >= 10:\n",
        "                        logging.info(f\"Parada antecipada no cliente {c+1}, passo {step+1}.\")\n",
        "                        break\n",
        "\n",
        "                if best_local_weights is None:\n",
        "                    best_local_weights = local_weights\n",
        "\n",
        "                mechanism = Gaussian()\n",
        "                mechanism.set_variance(dp_noise**2)\n",
        "                noisy_weights = mechanism(best_local_weights)\n",
        "                client_updates.append(noisy_weights)\n",
        "\n",
        "                epsilon = dp_noise\n",
        "                delta = 1e-5\n",
        "                epsilons.append(epsilon)\n",
        "                deltas.append(delta)\n",
        "\n",
        "                client_update_path = os.path.join(update_dir, f\"client_{c+1}_update_round_{r+1}.npy\")\n",
        "                np.save(client_update_path, noisy_weights)\n",
        "                logging.info(f\"Atualização do Cliente {c+1} salva em: {client_update_path}\")\n",
        "\n",
        "            global_weights = federated_averaging(global_weights, client_updates)\n",
        "            logging.info(f\"Agregação concluída na rodada {r+1} (Quântico)\")\n",
        "\n",
        "            global_weights_path = os.path.join(update_dir, f\"global_weights_round_{r+1}.npy\")\n",
        "            np.save(global_weights_path, global_weights)\n",
        "            logging.info(f\"Pesos globais após a rodada {r+1} salvos em: {global_weights_path}\")\n",
        "\n",
        "            privacy_metrics[r+1] = {\n",
        "                'epsilon': np.mean(epsilons),\n",
        "                'delta': np.mean(deltas)\n",
        "            }\n",
        "            logging.info(f\"Rodada {r+1} - ε={np.mean(epsilons):.4f}, δ={np.mean(deltas):.4f}\")\n",
        "\n",
        "        return global_weights, privacy_metrics\n",
        "\n",
        "    global_weights = copy.deepcopy(weights)\n",
        "    global_weights, privacy_metrics = train_federated(\n",
        "        global_weights, client_data,\n",
        "        rounds=rounds, local_steps=local_steps,\n",
        "        lambda_reg=lambda_reg, dp_noise=dp_noise, quantum_gates='RY'\n",
        "    )\n",
        "\n",
        "    def evaluate_model(weights, client_data, label_encoder, save_dir=data_dir, quantum_gates='RY'):\n",
        "        for c in range(len(client_data)):\n",
        "            X_tr_q, y_tr_q, X_val_q, y_val_q = client_data[c]\n",
        "            X_test_q = X_val_q\n",
        "            y_test_q = y_val_q\n",
        "\n",
        "            y_test_pred_probs = [(circuit_mera(weights, x, quantum_gates) + 1)/2 for x in X_test_q]\n",
        "            y_test_pred = [1 if prob > 0.5 else 0 for prob in y_test_pred_probs]\n",
        "\n",
        "            logging.info(f\"\\nCliente {c+1} - Modelo Quântico\")\n",
        "            print(classification_report(y_test_q, y_test_pred, target_names=label_encoder.classes_, zero_division=1))\n",
        "            try:\n",
        "                auc_test_q = roc_auc_score(y_test_q, y_test_pred_probs)\n",
        "                logging.info(f\"AUC-ROC (Quântico)={auc_test_q:.4f}\")\n",
        "            except ValueError as e:\n",
        "                logging.error(f\"Erro no ROC Quântico: {e}\")\n",
        "            print(confusion_matrix(y_test_q, y_test_pred))\n",
        "\n",
        "            test_df_q = pd.DataFrame({\n",
        "                \"features\": [x.tolist() for x in X_test_q],\n",
        "                \"labels\": y_test_q,\n",
        "                \"predictions\": y_test_pred,\n",
        "                \"predictions_probs\": y_test_pred_probs,\n",
        "                \"residuals\": [y - p for y, p in zip(y_test_q, y_test_pred)]\n",
        "            })\n",
        "            test_res_path = os.path.join(save_dir, f\"client_{c+1}_test_results_quantum.csv\")\n",
        "            test_df_q.to_csv(test_res_path, index=False)\n",
        "            logging.info(f\"Teste Quântico Cliente {c+1} salvo em: {test_res_path}\")\n",
        "\n",
        "    evaluate_model(global_weights, client_data, label_encoder)\n",
        "\n",
        "    final_q_weights_path = os.path.join(update_dir, \"global_weights_final.npy\")\n",
        "    np.save(final_q_weights_path, global_weights)\n",
        "    logging.info(f\"Pesos globais finais (Quântico) em: {final_q_weights_path}\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # 13. Modelo Clássico (CNN) e Aprendizado Federado\n",
        "    # =============================================================================\n",
        "\n",
        "    def create_cnn_model(input_shape=(64,), num_classes=2):\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Reshape((8, 8, 1), input_shape=input_shape))\n",
        "        model.add(layers.Conv2D(32, (3,3), activation='relu', name='conv2d_1'))\n",
        "        model.add(layers.MaxPooling2D((2,2), name='maxpool_1'))\n",
        "        model.add(layers.Conv2D(64, (3,3), activation='relu', name='conv2d_2'))\n",
        "        model.add(layers.MaxPooling2D((2,2), name='maxpool_2'))\n",
        "        model.add(layers.Conv2D(64, (3,3), activation='relu', name='conv2d_3'))\n",
        "        model.add(layers.Flatten(name='flatten'))\n",
        "        model.add(layers.Dense(64, activation='relu', name='dense_1'))\n",
        "        model.add(layers.Dense(num_classes, activation='softmax', name='output'))\n",
        "        return model\n",
        "\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "\n",
        "    def prepare_cnn_data(X, y, num_classes):\n",
        "        X = X.reshape(-1, 8, 8, 1).astype('float32')\n",
        "        y = tf.keras.utils.to_categorical(y, num_classes)\n",
        "        return X, y\n",
        "\n",
        "    X_train_full_cnn, y_train_full_cnn = prepare_cnn_data(X_train_full, y_train_full, num_classes)\n",
        "    X_test_cnn, y_test_cnn = prepare_cnn_data(X_test, y_test, num_classes)\n",
        "\n",
        "    client_data_cnn = []\n",
        "    X_split_cnn = np.array_split(X_train_full_cnn, num_clients)\n",
        "    y_split_cnn = np.array_split(y_train_full_cnn, num_clients)\n",
        "    for i in range(num_clients):\n",
        "        client_data_cnn.append((X_split_cnn[i], y_split_cnn[i]))\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        X_tr_cnn, X_val_cnn, y_tr_cnn, y_val_cnn = train_test_split(\n",
        "            client_data_cnn[i][0], client_data_cnn[i][1],\n",
        "            test_size=0.1, random_state=42,\n",
        "            stratify=np.argmax(client_data_cnn[i][1], axis=1)\n",
        "        )\n",
        "        client_data_cnn[i] = (X_tr_cnn, y_tr_cnn, X_val_cnn, y_val_cnn)\n",
        "\n",
        "    logging.info(f\"Cliente 1 (CNN) -> Treino: {client_data_cnn[0][0].shape[0]}, Val: {client_data_cnn[0][2].shape[0]}\")\n",
        "\n",
        "    cnn_model = create_cnn_model((64,), num_classes)\n",
        "    cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    def train_federated_cnn(global_weights_cnn, client_data_cnn, rounds=5, epochs=5, dp_noise=0.05):\n",
        "        privacy_metrics_cnn = {}\n",
        "        for r in range(rounds):\n",
        "            logging.info(f\"\\n--- Rodada Federada {r+1}/{rounds} - CNN ---\")\n",
        "            client_updates_cnn = []\n",
        "            epsilons_cnn = []\n",
        "            deltas_cnn = []\n",
        "            for c in range(len(client_data_cnn)):\n",
        "                logging.info(f\"Treinando Cliente {c+1} (CNN)\")\n",
        "                local_model = create_cnn_model((64,), num_classes)\n",
        "                local_model.set_weights(global_weights_cnn)\n",
        "\n",
        "                X_tr_cnn, y_tr_cnn, X_val_cnn, y_val_cnn = client_data_cnn[c]\n",
        "                early_stop = EarlyStopping(monitor='val_loss', patience=5, min_delta=1e-4, restore_best_weights=True)\n",
        "                history = local_model.fit(\n",
        "                    X_tr_cnn, y_tr_cnn,\n",
        "                    epochs=epochs, batch_size=32,\n",
        "                    validation_data=(X_val_cnn, y_val_cnn),\n",
        "                    callbacks=[early_stop], verbose=0\n",
        "                )\n",
        "                logging.info(f\"Cliente {c+1} - CNN - final val_loss={history.history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "                trained_weights = local_model.get_weights()\n",
        "                noisy_weights_cnn = []\n",
        "                for layer_w in trained_weights:\n",
        "                    noise = np.random.normal(0, dp_noise, layer_w.shape)\n",
        "                    noisy_weights_cnn.append(layer_w + noise)\n",
        "                client_updates_cnn.append(noisy_weights_cnn)\n",
        "\n",
        "                epsilon_cnn = dp_noise\n",
        "                delta_cnn = 1e-5\n",
        "                epsilons_cnn.append(epsilon_cnn)\n",
        "                deltas_cnn.append(delta_cnn)\n",
        "\n",
        "                client_path_cnn = os.path.join(update_dir, f\"client_{c+1}_cnn_update_round_{r+1}.npy\")\n",
        "                np.save(client_path_cnn, noisy_weights_cnn)\n",
        "                logging.info(f\"Atualização CNN Cliente {c+1} salva em: {client_path_cnn}\")\n",
        "\n",
        "            # Agregação\n",
        "            aggregated_weights_cnn = []\n",
        "            for layer_idx in range(len(global_weights_cnn)):\n",
        "                layer_agg = []\n",
        "                for w in range(len(client_updates_cnn[0][layer_idx])):\n",
        "                    w_layer = [client_updates_cnn[c][layer_idx][w] for c in range(len(client_updates_cnn))]\n",
        "                    layer_agg.append(np.mean(w_layer, axis=0))\n",
        "                aggregated_weights_cnn.append(layer_agg)\n",
        "\n",
        "            global_weights_cnn = aggregated_weights_cnn\n",
        "            logging.info(f\"Agregação concluída na rodada {r+1} - CNN\")\n",
        "\n",
        "            global_weights_path_cnn = os.path.join(update_dir, f\"global_weights_cnn_round_{r+1}.npy\")\n",
        "            np.save(global_weights_path_cnn, global_weights_cnn)\n",
        "            logging.info(f\"Pesos globais salvos em: {global_weights_path_cnn}\")\n",
        "\n",
        "            privacy_metrics_cnn[r+1] = {'epsilon': np.mean(epsilons_cnn), 'delta': np.mean(deltas_cnn)}\n",
        "            logging.info(f\"Métricas Privacidade (CNN) R{r+1}: ε={np.mean(epsilons_cnn):.4f}, δ={np.mean(deltas_cnn):.4f}\")\n",
        "\n",
        "        return global_weights_cnn, privacy_metrics_cnn\n",
        "\n",
        "    global_weights_cnn = cnn_model.get_weights()\n",
        "    global_weights_cnn, privacy_metrics_cnn = train_federated_cnn(\n",
        "        global_weights_cnn, client_data_cnn,\n",
        "        rounds=5, epochs=5, dp_noise=dp_noise\n",
        "    )\n",
        "\n",
        "    def evaluate_model_cnn(global_weights_cnn, client_data_cnn, label_encoder, save_dir=data_dir):\n",
        "        for c in range(len(client_data_cnn)):\n",
        "            X_tr_cnn, y_tr_cnn, X_val_cnn, y_val_cnn = client_data_cnn[c]\n",
        "            model_cnn_eval = create_cnn_model((64,), num_classes)\n",
        "            model_cnn_eval.set_weights(global_weights_cnn)\n",
        "\n",
        "            y_pred_probs_cnn = model_cnn_eval.predict(X_val_cnn).flatten()\n",
        "            y_pred_cnn = np.argmax(model_cnn_eval.predict(X_val_cnn), axis=1)\n",
        "            logging.info(f\"\\nCliente {c+1} - CNN\")\n",
        "            print(classification_report(np.argmax(y_val_cnn, axis=1), y_pred_cnn,\n",
        "                                        target_names=label_encoder.classes_, zero_division=1))\n",
        "            try:\n",
        "                auc_test_cnn = roc_auc_score(np.argmax(y_val_cnn, axis=1), y_pred_probs_cnn)\n",
        "                logging.info(f\"AUC-ROC (CNN)={auc_test_cnn:.4f}\")\n",
        "            except ValueError as e:\n",
        "                logging.error(f\"Erro no AUC (CNN): {e}\")\n",
        "\n",
        "            print(confusion_matrix(np.argmax(y_val_cnn, axis=1), y_pred_cnn))\n",
        "\n",
        "            test_features_cnn = [x.tolist() for x in X_val_cnn]\n",
        "            test_df_cnn = pd.DataFrame({\n",
        "                \"features\": test_features_cnn,\n",
        "                \"labels\": np.argmax(y_val_cnn, axis=1),\n",
        "                \"predictions\": y_pred_cnn,\n",
        "                \"predictions_probs\": y_pred_probs_cnn,\n",
        "                \"residuals\": [y - p for y, p in zip(np.argmax(y_val_cnn, axis=1), y_pred_cnn)]\n",
        "            })\n",
        "            csv_path_cnn = os.path.join(save_dir, f\"client_{c+1}_test_results_cnn.csv\")\n",
        "            test_df_cnn.to_csv(csv_path_cnn, index=False)\n",
        "            logging.info(f\"Resultados CNN salvos em: {csv_path_cnn}\")\n",
        "\n",
        "    evaluate_model_cnn(global_weights_cnn, client_data_cnn, label_encoder)\n",
        "\n",
        "    final_cnn_path = os.path.join(update_dir, \"global_weights_final_cnn.npy\")\n",
        "    np.save(final_cnn_path, global_weights_cnn)\n",
        "    logging.info(f\"Pesos globais finais do CNN salvos em: {final_cnn_path}\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # 13. Visualizações Adicionais (Scatter / Histogram Residuals)\n",
        "    # =============================================================================\n",
        "\n",
        "    def plot_scatter_predictions_quantum(client_data, weights, label_encoder, save_dir=data_dir, quantum_gates='RY'):\n",
        "        for c in range(len(client_data)):\n",
        "            X_tr, y_tr, X_val, y_val = client_data[c]\n",
        "            y_val_pred_probs = [(circuit_mera(weights, x, quantum_gates)+1)/2 for x in X_val]\n",
        "            y_val_pred = [1 if prob>0.5 else 0 for prob in y_val_pred_probs]\n",
        "            plt.figure(figsize=(10,6))\n",
        "            plt.scatter(y_val, y_val_pred_probs, alpha=0.7, label=f\"Cliente {c+1}\")\n",
        "            plt.title(f\"Scatter: Prob vs Rótulo (Cliente {c+1}) - Quântico\")\n",
        "            plt.xlabel(\"Rótulo Verdadeiro\")\n",
        "            plt.ylabel(\"Prob (Classe=1)\")\n",
        "            plt.legend()\n",
        "            sp_path = os.path.join(save_dir, f\"scatter_quantum_client_{c+1}.png\")\n",
        "            plt.savefig(sp_path)\n",
        "            plt.show()\n",
        "            logging.info(f\"Scatter Quântico salvo em: {sp_path}\")\n",
        "\n",
        "    def plot_scatter_predictions_cnn(client_data_cnn, weights_cnn, label_encoder, save_dir=data_dir):\n",
        "        for c in range(len(client_data_cnn)):\n",
        "            X_tr, y_tr, X_val, y_val = client_data_cnn[c]\n",
        "            cnn_eval = create_cnn_model((64,), num_classes)\n",
        "            cnn_eval.set_weights(weights_cnn)\n",
        "            y_val_pred_probs_cnn = cnn_eval.predict(X_val).flatten()\n",
        "            y_val_pred_cnn = np.argmax(cnn_eval.predict(X_val), axis=1)\n",
        "\n",
        "            plt.figure(figsize=(10,6))\n",
        "            plt.scatter(np.argmax(y_val, axis=1), y_val_pred_probs_cnn, alpha=0.7, label=f\"Cliente {c+1}\")\n",
        "            plt.title(f\"Scatter: Prob vs Rótulo (Cliente {c+1}) - CNN\")\n",
        "            plt.xlabel(\"Rótulo Verdadeiro\")\n",
        "            plt.ylabel(\"Prob (Classe=1)\")\n",
        "            plt.legend()\n",
        "            sp_path_cnn = os.path.join(save_dir, f\"scatter_cnn_client_{c+1}.png\")\n",
        "            plt.savefig(sp_path_cnn)\n",
        "            plt.show()\n",
        "            logging.info(f\"Scatter CNN salvo em: {sp_path_cnn}\")\n",
        "\n",
        "    def plot_residuals_histogram_quantum(client_data, weights, save_dir=data_dir, quantum_gates='RY'):\n",
        "        for c in range(len(client_data)):\n",
        "            X_tr, y_tr, X_val, y_val = client_data[c]\n",
        "            y_val_probs = [(circuit_mera(weights, x, quantum_gates)+1)/2 for x in X_val]\n",
        "            y_val_pred = [1 if prob>0.5 else 0 for prob in y_val_probs]\n",
        "            residuals = [y - p for y, p in zip(y_val, y_val_pred)]\n",
        "            plt.figure(figsize=(10,6))\n",
        "            plt.hist(residuals, bins=20, alpha=0.7)\n",
        "            plt.title(f\"Hist Resíduos Cliente {c+1} - Quântico\")\n",
        "            plt.xlabel(\"Resíduo (Real - Predito)\")\n",
        "            plt.ylabel(\"Frequência\")\n",
        "            hr_path = os.path.join(save_dir, f\"hist_res_quantum_client_{c+1}.png\")\n",
        "            plt.savefig(hr_path)\n",
        "            plt.show()\n",
        "            logging.info(f\"Hist de Resíduos Quântico salvo em: {hr_path}\")\n",
        "\n",
        "    def plot_residuals_histogram_cnn(client_data_cnn, weights_cnn, save_dir=data_dir):\n",
        "        for c in range(len(client_data_cnn)):\n",
        "            X_tr, y_tr, X_val, y_val = client_data_cnn[c]\n",
        "            model_cnn_eval = create_cnn_model((64,), num_classes)\n",
        "            model_cnn_eval.set_weights(weights_cnn)\n",
        "            y_val_pred_cnn = np.argmax(model_cnn_eval.predict(X_val), axis=1)\n",
        "            y_val_labels = np.argmax(y_val, axis=1)\n",
        "            residuals_cnn = [y - p for y, p in zip(y_val_labels, y_val_pred_cnn)]\n",
        "            plt.figure(figsize=(10,6))\n",
        "            plt.hist(residuals_cnn, bins=20, alpha=0.7)\n",
        "            plt.title(f\"Hist Resíduos Cliente {c+1} - CNN\")\n",
        "            plt.xlabel(\"Resíduo (Real - Predito)\")\n",
        "            plt.ylabel(\"Frequência\")\n",
        "            hr_cnn_path = os.path.join(save_dir, f\"hist_res_cnn_client_{c+1}.png\")\n",
        "            plt.savefig(hr_cnn_path)\n",
        "            plt.show()\n",
        "            logging.info(f\"Hist de Resíduos CNN salvo em: {hr_cnn_path}\")\n",
        "\n",
        "    plot_scatter_predictions_quantum(client_data, global_weights, label_encoder, save_dir=data_dir)\n",
        "    plot_scatter_predictions_cnn(client_data_cnn, global_weights_cnn, label_encoder, save_dir=data_dir)\n",
        "    plot_residuals_histogram_quantum(client_data, global_weights, save_dir=data_dir)\n",
        "    plot_residuals_histogram_cnn(client_data_cnn, global_weights_cnn, save_dir=data_dir)\n",
        "\n",
        "    # =============================================================================\n",
        "    # 14. Classificação de Imagem do Usuário c/ Grad-CAM Simplificado (CNN)\n",
        "    # =============================================================================\n",
        "\n",
        "    def classify_user_image_both_models(model_weights_quantum, pca, label_encoder,\n",
        "                                        global_weights_cnn, save_dir=data_dir,\n",
        "                                        quantum_gates='RY'):\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"Nenhuma imagem foi carregada.\")\n",
        "            return\n",
        "\n",
        "        for fn in uploaded.keys():\n",
        "            img_path = fn\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img_resized = img.resize((64,64))\n",
        "            img_array = np.array(img_resized).flatten()/255.0\n",
        "            # PCA\n",
        "            img_pca = pca.transform([img_array])[0]\n",
        "\n",
        "            # Modelo Quântico\n",
        "            pred_quantum = circuit_mera(model_weights_quantum, img_pca, quantum_gates)\n",
        "            prob_quantum = (pred_quantum+1)/2\n",
        "            class_quantum = 1 if prob_quantum>0.5 else 0\n",
        "            conf_quantum = prob_quantum if class_quantum==1 else 1 - prob_quantum\n",
        "            label_q = label_encoder.inverse_transform([class_quantum])[0]\n",
        "\n",
        "            # Modelo Clássico (CNN)\n",
        "            model_cnn_user = create_cnn_model((64,), num_classes)\n",
        "            model_cnn_user.set_weights(global_weights_cnn)\n",
        "            img_cnn = img_pca.reshape(1,64).reshape(-1,8,8,1)\n",
        "            y_pred_probs_cnn = model_cnn_user.predict(img_cnn).flatten()\n",
        "            y_pred_cnn = np.argmax(y_pred_probs_cnn)\n",
        "            conf_cnn = y_pred_probs_cnn[y_pred_cnn]\n",
        "            label_cnn = label_encoder.inverse_transform([y_pred_cnn])[0]\n",
        "\n",
        "            print(f\"\\nImagem: {fn}\")\n",
        "            print(\"==> Modelo Quântico:\")\n",
        "            print(f\"Classe Predita: {label_q}\")\n",
        "            print(f\"Grau de Confiança: {conf_quantum:.4f}\")\n",
        "\n",
        "            print(\"==> Modelo Clássico (CNN):\")\n",
        "            print(f\"Classe Predita: {label_cnn}\")\n",
        "            print(f\"Grau de Confiança: {conf_cnn:.4f}\")\n",
        "\n",
        "            # Grad-CAM via tf-keras-vis\n",
        "            try:\n",
        "                conv_layer = None\n",
        "                for layer in reversed(model_cnn_user.layers):\n",
        "                    if isinstance(layer, layers.Conv2D):\n",
        "                        conv_layer = layer\n",
        "                        break\n",
        "                if conv_layer is None:\n",
        "                    raise ValueError(\"Nenhuma camada convolucional encontrada para Grad-CAM.\")\n",
        "\n",
        "                score_cnn = CategoricalScore(y_pred_cnn)\n",
        "                gradcam = Gradcam(model_cnn_user, clone=False)\n",
        "                heatmap = gradcam(score_cnn, [img_cnn], penultimate_layer=conv_layer.name)[0]\n",
        "\n",
        "                plt.figure(figsize=(12,6))\n",
        "                plt.subplot(1,2,1)\n",
        "                plt.imshow(img_resized)\n",
        "                plt.title(\"Imagem Original\")\n",
        "                plt.axis(\"off\")\n",
        "\n",
        "                plt.subplot(1,2,2)\n",
        "                plt.imshow(img_resized, alpha=0.6)\n",
        "                plt.imshow(heatmap, cmap='jet', alpha=0.4)\n",
        "                plt.title(f\"Grad-CAM - CNN\\nClasse: {label_cnn} | Confiança: {conf_cnn:.2f}\")\n",
        "                plt.axis(\"off\")\n",
        "\n",
        "                gradcam_path = os.path.join(save_dir, f\"user_image_gradcam_{fn}\")\n",
        "                plt.savefig(gradcam_path)\n",
        "                plt.show()\n",
        "                logging.info(f\"Grad-CAM salvo em: {gradcam_path}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Erro ao gerar Grad-CAM: {e}\")\n",
        "\n",
        "    # Se desejar classificar imagens do usuário (upload)\n",
        "    classify_user_image_both_models(global_weights, pca, label_encoder, global_weights_cnn)\n",
        "\n",
        "    # =============================================================================\n",
        "    # 15. Testes Unitários de Exemplo\n",
        "    # =============================================================================\n",
        "\n",
        "    def test_federated_averaging():\n",
        "        \"\"\"Exemplo de teste unitário para federated_averaging.\"\"\"\n",
        "        global_w = np.array([[[0.1, 0.2],[0.3,0.4]], [[0.5,0.6],[0.7,0.8]]])\n",
        "        client_w = [\n",
        "            np.array([[[0.11,0.21],[0.31,0.41]], [[0.51,0.61],[0.71,0.81]]]),\n",
        "            np.array([[[0.12,0.22],[0.32,0.42]], [[0.52,0.62],[0.72,0.82]]])\n",
        "        ]\n",
        "        expected = np.array([[[0.115,0.215],[0.315,0.415]], [[0.515,0.615],[0.715,0.815]]])\n",
        "        avg_w = federated_averaging(global_w, client_w)\n",
        "        assert np.allclose(avg_w, expected), \"federated_averaging falhou no teste unitário!\"\n",
        "        logging.info(\"Teste de federated_averaging passou com sucesso.\")\n",
        "\n",
        "    test_federated_averaging()\n",
        "    logging.info(\"Processo finalizado com sucesso.\")\n",
        "\n",
        "else:\n",
        "    logging.error(\"Encerrando o processo devido à falta do arquivo ZIP.\")\n"
      ],
      "metadata": {
        "id": "cNkqAFTfzTAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}